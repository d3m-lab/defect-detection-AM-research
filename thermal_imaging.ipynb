{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "e_YtxS4MHR7Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import time\n",
    "from PIL import Image as PILImage\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psutil\n",
    "import gc\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as tv_models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "id": "J2XCA6hFHSmA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Settings"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "MODEL_DIR = str(BASE_DIR / \"ml_models\")\n",
    "data_dir = str(BASE_DIR / \"Processed_Data\")\n",
    "AUGMENTED_IMAGES_DIR = str(BASE_DIR / \"aug_images\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "resnet_model_training = True\n",
    "mobilenet_model_training = True\n",
    "efficientnet_model_training = True\n",
    "vgg16_model_training = True\n",
    "alexnet_model_training = True\n",
    "hybrid_model_training = True\n",
    "cnn_model_training = True\n",
    "\n",
    "common_epochs = 1\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC9cQ6nyubOR"
   },
   "source": "# GPU Check"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U4_FH8aOua0p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "12e8da8b-eb22-414a-e64b-7f75badd4529"
   },
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znhjbGhbFr_d"
   },
   "source": [
    "# Clean GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7aqOnfeFr_d",
    "outputId": "e9c7be82-2646-4785-e548-596fffb722ec"
   },
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8S2DEhEFr_d"
   },
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ep2rCAYgFr_d",
    "outputId": "11c8fd79-f78c-4f4b-ed96-7c34df833968"
   },
   "source": [
    "class ThermalDataExporter:\n",
    "    def __init__(self, output_dir='thermal_analysis_results'):\n",
    "        self.output_dir = output_dir\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.training_data = {}\n",
    "        self.model_evaluations = []\n",
    "        self.confusion_matrices = {}\n",
    "        print(f'ThermalDataExporter initialized. Output directory: {output_dir}')\n",
    "\n",
    "    def save_training_curves(self, model_name, train_losses, train_accs, val_losses, val_accs):\n",
    "        \"\"\"Save training curves data for a specific model\"\"\"\n",
    "        self.training_data[model_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs\n",
    "        }\n",
    "        print(f'Training curves saved for: {model_name}')\n",
    "\n",
    "    def save_model_evaluation(self, model_name, accuracy, precision, recall, f1_score,\n",
    "                            confusion_matrix, classification_report_text):\n",
    "        \"\"\"Save comprehensive model evaluation metrics\"\"\"\n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'timestamp': self.timestamp,\n",
    "            'classification_report': classification_report_text\n",
    "        }\n",
    "\n",
    "        self.model_evaluations.append(result)\n",
    "        self.confusion_matrices[model_name] = confusion_matrix\n",
    "        print(f'Model evaluation saved for: {model_name}')\n",
    "        return result\n",
    "\n",
    "    def export_training_curves(self, training_data):\n",
    "        \"\"\"Export training curves data to CSV\"\"\"\n",
    "        df = pd.DataFrame(training_data)\n",
    "        filename = f'training_curves_{self.timestamp}.csv'\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f'Training curves exported to: {filepath}')\n",
    "        return filepath\n",
    "\n",
    "    def export_model_evaluations(self, model_results):\n",
    "        \"\"\"Export model evaluation results to CSV\"\"\"\n",
    "        df = pd.DataFrame(model_results)\n",
    "        filename = f'model_evaluations_{self.timestamp}.csv'\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f'Model evaluations exported to: {filepath}')\n",
    "        return filepath\n",
    "\n",
    "    def save_all_results_csv(self):\n",
    "        \"\"\"Export all collected data to CSV files\"\"\"\n",
    "        # Export training data\n",
    "        for model_name, data in self.training_data.items():\n",
    "            df = pd.DataFrame({\n",
    "                'epoch': range(1, len(data['train_losses']) + 1),\n",
    "                'train_loss': data['train_losses'],\n",
    "                'train_accuracy': data['train_accs'],\n",
    "                'val_loss': data['val_losses'],\n",
    "                'val_accuracy': data['val_accs']\n",
    "            })\n",
    "            filename = f'{model_name}_training_curves_{self.timestamp}.csv'\n",
    "            filepath = os.path.join(self.output_dir, filename)\n",
    "            df.to_csv(filepath, index=False)\n",
    "            print(f'{model_name} training curves exported to: {filepath}')\n",
    "\n",
    "        # Export model evaluations\n",
    "        if self.model_evaluations:\n",
    "            df_eval = pd.DataFrame(self.model_evaluations)\n",
    "            filename = f'model_evaluations_{self.timestamp}.csv'\n",
    "            filepath = os.path.join(self.output_dir, filename)\n",
    "            df_eval.to_csv(filepath, index=False)\n",
    "            print(f'Model evaluations exported to: {filepath}')\n",
    "\n",
    "        print('All results exported to CSV files!')\n",
    "\n",
    "print('ThermalDataExporter class loaded successfully!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngsSitLQFr_d"
   },
   "source": [
    "## Data Analysis Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0fggX7JFr_d",
    "outputId": "8a0b7b01-ccc4-4f22-b21a-3812106093e8"
   },
   "source": [
    "# Enhanced Data Visualization and CSV Export Functions\n",
    "\n",
    "class EnhancedDataExporter:\n",
    "    \"\"\"Enhanced data exporter with comprehensive CSV and visualization capabilities\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir='thermal_analysis_results'):\n",
    "        self.output_dir = output_dir\n",
    "        self.timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f'EnhancedDataExporter initialized. Output directory: {output_dir}')\n",
    "\n",
    "        # Set seaborn style\n",
    "        sns.set_style('whitegrid')\n",
    "        sns.set_palette('husl')\n",
    "        plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "    def export_training_data_to_csv(self, all_training_data):\n",
    "        \"\"\"Export comprehensive training data for all models to CSV\"\"\"\n",
    "        print('Exporting comprehensive training data to CSV...')\n",
    "\n",
    "        for model_name, data in all_training_data.items():\n",
    "            df = pd.DataFrame({\n",
    "                'epoch': range(1, len(data['train_losses']) + 1),\n",
    "                'train_loss': data['train_losses'],\n",
    "                'train_accuracy': data['train_accs'],\n",
    "                'val_loss': data['val_losses'],\n",
    "                'val_accuracy': data['val_accs'],\n",
    "                'model_name': model_name,\n",
    "                'timestamp': self.timestamp\n",
    "            })\n",
    "\n",
    "            filename = f'{model_name.lower().replace(\" \", \"_\")}_training_data_{self.timestamp}.csv'\n",
    "            filepath = os.path.join(self.output_dir, filename)\n",
    "            df.to_csv(filepath, index=False)\n",
    "            print(f'  {model_name} training data: {filepath}')\n",
    "\n",
    "    def export_model_comparison_csv(self, model_results):\n",
    "        \"\"\"Export comprehensive model comparison data\"\"\"\n",
    "        print('Exporting model comparison data to CSV...')\n",
    "\n",
    "        comparison_data = []\n",
    "        for result in model_results:\n",
    "            comparison_data.append({\n",
    "                'model_name': result['model_name'],\n",
    "                'accuracy': result.get('accuracy', 0),\n",
    "                'precision_macro': result.get('precision_macro', 0),\n",
    "                'precision_weighted': result.get('precision_weighted', 0),\n",
    "                'recall_macro': result.get('recall_macro', 0),\n",
    "                'recall_weighted': result.get('recall_weighted', 0),\n",
    "                'f1_macro': result.get('f1_macro', 0),\n",
    "                'f1_weighted': result.get('f1_weighted', 0),\n",
    "                'timestamp': self.timestamp\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        filename = f'model_comparison_{self.timestamp}.csv'\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f'  Model comparison data: {filepath}')\n",
    "        return df\n",
    "\n",
    "    def export_confusion_matrices_csv(self, model_results, class_names):\n",
    "        \"\"\"Export confusion matrices data to CSV\"\"\"\n",
    "        print('Exporting confusion matrices to CSV...')\n",
    "\n",
    "        for result in model_results:\n",
    "            if 'confusion_matrix' in result:\n",
    "                cm = result['confusion_matrix']\n",
    "                df = pd.DataFrame(cm, columns=class_names, index=class_names)\n",
    "                df.index.name = 'True_Label'\n",
    "                df['model_name'] = result['model_name']\n",
    "                df['timestamp'] = self.timestamp\n",
    "\n",
    "                filename = f'confusion_matrix_{result[\"model_name\"].lower().replace(\" \", \"_\")}_{self.timestamp}.csv'\n",
    "                filepath = os.path.join(self.output_dir, filename)\n",
    "                df.to_csv(filepath)\n",
    "                print(f'  {result[\"model_name\"]} confusion matrix: {filepath}')\n",
    "\n",
    "def plot_enhanced_training_curves(all_training_data, save_individual=True):\n",
    "    \"\"\"Enhanced training curves with seaborn styling\"\"\"\n",
    "    print('Creating enhanced training curves visualization...')\n",
    "\n",
    "    if save_individual:\n",
    "        # Individual plots for each model\n",
    "        for model_name, data in all_training_data.items():\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            epochs = range(1, len(data['train_losses']) + 1)\n",
    "\n",
    "            # Loss plot\n",
    "            axes[0, 0].plot(epochs, data['train_losses'], 'o-', label='Training', linewidth=2, markersize=5)\n",
    "            axes[0, 0].plot(epochs, data['val_losses'], 's-', label='Validation', linewidth=2, markersize=5)\n",
    "            axes[0, 0].set_title(f'{model_name} - Loss Over Time', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('Epoch')\n",
    "            axes[0, 0].set_ylabel('Loss')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "            # Accuracy plot\n",
    "            axes[0, 1].plot(epochs, [acc * 100 for acc in data['train_accs']], 'o-', label='Training', linewidth=2, markersize=5)\n",
    "            axes[0, 1].plot(epochs, [acc * 100 for acc in data['val_accs']], 's-', label='Validation', linewidth=2, markersize=5)\n",
    "            axes[0, 1].set_title(f'{model_name} - Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "            axes[0, 1].set_xlabel('Epoch')\n",
    "            axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "            # Loss difference\n",
    "            loss_diff = np.array(data['val_losses']) - np.array(data['train_losses'])\n",
    "            axes[1, 0].plot(epochs, loss_diff, 'o-', color='red', linewidth=2, markersize=5)\n",
    "            axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            axes[1, 0].set_title(f'{model_name} - Overfitting Analysis (Val - Train Loss)', fontsize=14, fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Loss Difference')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "            # Learning rate effect (if available)\n",
    "            axes[1, 1].plot(epochs, data['val_accs'], 'o-', label='Validation Accuracy', linewidth=2, markersize=5)\n",
    "            axes[1, 1].set_title(f'{model_name} - Validation Accuracy Trend', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            filename = f'enhanced_training_curves_{model_name.lower().replace(\" \", \"_\")}.png'\n",
    "            plt.savefig(os.path.join('thermal_analysis_results', filename), dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f'  Enhanced training curves for {model_name} saved!')\n",
    "\n",
    "def plot_comprehensive_model_comparison(model_results):\n",
    "    \"\"\"Comprehensive model comparison with multiple visualization types\"\"\"\n",
    "    print('Creating comprehensive model comparison...')\n",
    "\n",
    "    if len(model_results) < 2:\n",
    "        print('Need at least 2 models for comparison')\n",
    "        return\n",
    "\n",
    "    # Prepare data for different metrics\n",
    "    metrics_data = []\n",
    "    for result in model_results:\n",
    "        metrics_data.append({\n",
    "            'Model': result['model_name'],\n",
    "            'Accuracy': result.get('accuracy', 0) * 100,\n",
    "            'Precision (Macro)': result.get('precision_macro', 0) * 100,\n",
    "            'Recall (Macro)': result.get('recall_macro', 0) * 100,\n",
    "            'F1-Score (Macro)': result.get('f1_macro', 0) * 100,\n",
    "            'Precision (Weighted)': result.get('precision_weighted', 0) * 100,\n",
    "            'Recall (Weighted)': result.get('recall_weighted', 0) * 100,\n",
    "            'F1-Score (Weighted)': result.get('f1_weighted', 0) * 100\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Create comprehensive comparison figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "    # 1. Bar plot for macro metrics\n",
    "    macro_cols = ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)']\n",
    "    df_macro = df[['Model'] + macro_cols].melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "    sns.barplot(data=df_macro, x='Metric', y='Score', hue='Model', ax=axes[0, 0], palette='Set2')\n",
    "    axes[0, 0].set_title('Model Performance Comparison (Macro Averages)', fontsize=16, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Score (%)')\n",
    "    axes[0, 0].legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # 2. Heatmap for all metrics\n",
    "    df_heatmap = df.set_index('Model').T\n",
    "    sns.heatmap(df_heatmap, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0, 1], cbar_kws={'label': 'Score (%)'})\n",
    "    axes[0, 1].set_title('Performance Heatmap (All Metrics)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 3. Radar/Spider plot for comprehensive view\n",
    "    angles = np.linspace(0, 2 * np.pi, len(macro_cols), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "\n",
    "    ax_radar = plt.subplot(2, 2, 3, projection='polar')\n",
    "    colors = sns.color_palette('Set2', len(model_results))\n",
    "\n",
    "    for idx, result in enumerate(model_results):\n",
    "        values = [\n",
    "            result.get('accuracy', 0) * 100,\n",
    "            result.get('precision_macro', 0) * 100,\n",
    "            result.get('recall_macro', 0) * 100,\n",
    "            result.get('f1_macro', 0) * 100\n",
    "        ]\n",
    "        values += values[:1]  # Complete the circle\n",
    "\n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, label=result['model_name'], color=colors[idx])\n",
    "        ax_radar.fill(angles, values, alpha=0.25, color=colors[idx])\n",
    "\n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "    ax_radar.set_ylim(0, 100)\n",
    "    ax_radar.set_title('Performance Radar Chart', fontsize=16, fontweight='bold', pad=30)\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax_radar.grid(True)\n",
    "\n",
    "    # 4. Box plot for metric distribution\n",
    "    metric_values = []\n",
    "    for result in model_results:\n",
    "        for metric_name, value in [('Accuracy', result.get('accuracy', 0) * 100),\n",
    "                                   ('Precision', result.get('precision_macro', 0) * 100),\n",
    "                                   ('Recall', result.get('recall_macro', 0) * 100),\n",
    "                                   ('F1-Score', result.get('f1_macro', 0) * 100)]:\n",
    "            metric_values.append({'Model': result['model_name'], 'Metric': metric_name, 'Value': value})\n",
    "\n",
    "    df_box = pd.DataFrame(metric_values)\n",
    "    sns.boxplot(data=df_box, x='Model', y='Value', hue='Metric', ax=axes[1, 1], palette='Set3')\n",
    "    axes[1, 1].set_title('Metric Distribution by Model', fontsize=16, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Score (%)')\n",
    "    axes[1, 1].legend(title='Metric')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join('thermal_analysis_results', 'comprehensive_model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('  Comprehensive model comparison saved!')\n",
    "\n",
    "def plot_roc_curves(model_probabilities, y_true, class_names, model_names):\n",
    "    \"\"\"Enhanced ROC curves plotting with seaborn styling for multi-class classification\"\"\"\n",
    "\n",
    "    # Set seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Binarize the output for multi-class ROC\n",
    "    y_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(model_names), figsize=(6*len(model_names), 5))\n",
    "    if len(model_names) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    colors = sns.color_palette(\"husl\", n_classes + 1)\n",
    "\n",
    "    for model_idx, (model_name, y_prob) in enumerate(zip(model_names, model_probabilities)):\n",
    "        ax = axes[model_idx]\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_prob.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Plot micro-average ROC curve with enhanced styling\n",
    "        ax.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "               color='deeppink', linestyle='--', linewidth=3,\n",
    "               label=f'Micro-avg ROC (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "\n",
    "        # Plot ROC curve for each class\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            ax.plot(fpr[i], tpr[i], color=color, linewidth=2.5,\n",
    "                   label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "        # Plot random classifier line\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.6, label='Random Classifier')\n",
    "\n",
    "        # Styling with seaborn\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name} - ROC Curves', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.legend(loc=\"lower right\", frameon=True, fancybox=True, shadow=True)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add subtle background color\n",
    "        ax.set_facecolor('#fafafa')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save with high quality\n",
    "    if not os.path.exists('thermal_analysis_results'):\n",
    "        os.makedirs('thermal_analysis_results')\n",
    "    plt.savefig(os.path.join('thermal_analysis_results', 'enhanced_roc_curves_seaborn.png'),\n",
    "               dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nROC AUC Summary:\")\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        y_prob = model_probabilities[model_idx]\n",
    "        fpr_micro, tpr_micro, _ = roc_curve(y_bin.ravel(), y_prob.ravel())\n",
    "        auc_micro = auc(fpr_micro, tpr_micro)\n",
    "        print(f\"  {model_name}: Micro-avg AUC = {auc_micro:.4f}\")\n",
    "def plot_precision_recall_curves(model_probabilities, y_true, class_names, model_names):\n",
    "    \"\"\"Plot Precision-Recall curves for multi-class classification\"\"\"\n",
    "    print('Creating Precision-Recall curves visualization...')\n",
    "\n",
    "    n_classes = len(class_names)\n",
    "    y_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for model_idx, (model_name, y_prob) in enumerate(zip(model_names, model_probabilities)):\n",
    "        if model_idx >= 4:\n",
    "            break\n",
    "\n",
    "        ax = axes[model_idx]\n",
    "\n",
    "        colors_local = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
    "        for i, color in zip(range(n_classes), colors_local):\n",
    "            precision, recall, _ = precision_recall_curve(y_bin[:, i], y_prob[:, i])\n",
    "            avg_precision = auc(recall, precision)\n",
    "\n",
    "            ax.plot(recall, precision, color=color, lw=2,\n",
    "                   label=f'{class_names[i]} (AP = {avg_precision:.2f})')\n",
    "\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title(f'Precision-Recall Curves - {model_name}', fontweight='bold')\n",
    "        ax.legend(loc=\"lower left\", fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(model_names), 4):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join('thermal_analysis_results', 'precision_recall_curves.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('  Precision-Recall curves saved!')\n",
    "\n",
    "def create_statistical_summary_report(model_results, output_dir='thermal_analysis_results'):\n",
    "    \"\"\"Create a comprehensive statistical summary report\"\"\"\n",
    "    print('Creating statistical summary report...')\n",
    "\n",
    "    # Prepare data for statistical analysis\n",
    "    stats_data = []\n",
    "    for result in model_results:\n",
    "        stats_data.append({\n",
    "            'Model': result['model_name'],\n",
    "            'Accuracy': result.get('accuracy', 0),\n",
    "            'Precision_Macro': result.get('precision_macro', 0),\n",
    "            'Precision_Weighted': result.get('precision_weighted', 0),\n",
    "            'Recall_Macro': result.get('recall_macro', 0),\n",
    "            'Recall_Weighted': result.get('recall_weighted', 0),\n",
    "            'F1_Macro': result.get('f1_macro', 0),\n",
    "            'F1_Weighted': result.get('f1_weighted', 0)\n",
    "        })\n",
    "\n",
    "    df_stats = pd.DataFrame(stats_data)\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    summary_stats = df_stats.describe()\n",
    "\n",
    "    # Find best performing model for each metric\n",
    "    best_models = {}\n",
    "    for col in df_stats.columns[1:]:  # Skip 'Model' column\n",
    "        best_idx = df_stats[col].idxmax()\n",
    "        best_models[col] = (df_stats.loc[best_idx, 'Model'], df_stats.loc[best_idx, col])\n",
    "\n",
    "    # Create and save summary report\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_file = os.path.join(output_dir, f'statistical_summary_report_{timestamp}.txt')\n",
    "\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(\"THERMAL IMAGING MODEL PERFORMANCE STATISTICAL SUMMARY\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"SUMMARY STATISTICS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(summary_stats.to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"BEST PERFORMING MODELS BY METRIC:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        for metric, (model, score) in best_models.items():\n",
    "            f.write(f\"{metric.replace('_', ' ').title()}: {model} ({score:.4f})\\n\")\n",
    "\n",
    "        f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"Report generated on: {pd.Timestamp.now()}\\n\")\n",
    "\n",
    "    print(f'  Statistical summary report saved: {report_file}')\n",
    "    return summary_stats, best_models\n",
    "\n",
    "# Global storage for enhanced data\n",
    "enhanced_training_data = {}\n",
    "enhanced_model_results = []\n",
    "\n",
    "print('Enhanced data visualization and CSV export functions loaded!')\n",
    "print('Available visualizations:')\n",
    "print('   • Enhanced training curves with overfitting analysis')\n",
    "print('   • Comprehensive model comparison (bar, heatmap, radar, box plots)')\n",
    "print('   • ROC curves for multi-class classification')\n",
    "print('   • Precision-Recall curves')\n",
    "print('   • Statistical summary reports')\n",
    "print('   • CSV export for all data types')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rqi_wIovFr_e",
    "outputId": "803e4ba5-e44e-4af9-dbd7-236ebe967064"
   },
   "source": [
    "# Enhanced ROC Curve Plotting with Additional Seaborn Features\n",
    "def plot_enhanced_roc_comparison(model_probabilities, y_true, class_names, model_names):\n",
    "    \"\"\"Creates a comprehensive ROC comparison plot with seaborn styling\"\"\"\n",
    "\n",
    "    # Set sophisticated seaborn theme\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Main ROC plot\n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    # Binarize labels\n",
    "    n_classes = len(class_names)\n",
    "    y_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "    # Color palette\n",
    "    colors = sns.color_palette(\"Set2\", len(model_names))\n",
    "\n",
    "    # Plot micro-average ROC for each model\n",
    "    auc_scores = []\n",
    "    for model_idx, (model_name, y_prob) in enumerate(zip(model_names, model_probabilities)):\n",
    "        fpr_micro, tpr_micro, _ = roc_curve(y_bin.ravel(), y_prob.ravel())\n",
    "        roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "        auc_scores.append(roc_auc_micro)\n",
    "\n",
    "        ax_main.plot(fpr_micro, tpr_micro,\n",
    "                    color=colors[model_idx],\n",
    "                    linewidth=3,\n",
    "                    label=f'{model_name} (AUC = {roc_auc_micro:.3f})')\n",
    "\n",
    "    # Random classifier line\n",
    "    ax_main.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.7, label='Random Classifier')\n",
    "\n",
    "    # Styling main plot\n",
    "    ax_main.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
    "    ax_main.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
    "    ax_main.set_title('ROC Curves Comparison - All Models (Micro-Average)',\n",
    "                     fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_main.legend(loc='lower right', fontsize=12, frameon=True)\n",
    "    ax_main.grid(True, alpha=0.3)\n",
    "    ax_main.set_facecolor('#fafafa')\n",
    "\n",
    "    # AUC comparison bar plot\n",
    "    ax_bar = fig.add_subplot(gs[1, 0])\n",
    "    bars = ax_bar.bar(model_names, auc_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax_bar.set_ylabel('AUC Score', fontsize=12, fontweight='bold')\n",
    "    ax_bar.set_title('AUC Comparison', fontsize=14, fontweight='bold')\n",
    "    ax_bar.set_ylim(0, 1)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, auc_scores):\n",
    "        height = bar.get_height()\n",
    "        ax_bar.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Performance metrics heatmap\n",
    "    ax_heat = fig.add_subplot(gs[1, 1])\n",
    "    metrics_data = np.array(auc_scores).reshape(1, -1)\n",
    "    sns.heatmap(metrics_data,\n",
    "               xticklabels=model_names,\n",
    "               yticklabels=['AUC'],\n",
    "               annot=True,\n",
    "               fmt='.3f',\n",
    "               cmap='RdYlGn',\n",
    "               ax=ax_heat,\n",
    "               cbar_kws={'label': 'AUC Score'})\n",
    "    ax_heat.set_title('AUC Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Enhanced ROC Analysis Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "    # Save the plot\n",
    "    if not os.path.exists('thermal_analysis_results'):\n",
    "        os.makedirs('thermal_analysis_results')\n",
    "    plt.savefig(os.path.join('thermal_analysis_results', 'enhanced_roc_dashboard_seaborn.png'),\n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    return auc_scores\n",
    "\n",
    "print(\"Enhanced ROC plotting functions loaded successfully!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AI7xtNjFr_e",
    "outputId": "c294cf1c-a4cd-4760-9792-d50534fe46a8"
   },
   "source": [
    "# Example usage of enhanced ROC plotting functions\n",
    "# This cell demonstrates how to use the enhanced ROC plotting capabilities\n",
    "\n",
    "def demonstrate_enhanced_roc_plotting(model_results_dict, test_probabilities, y_test, class_names):\n",
    "    \"\"\"\n",
    "    Demonstrates the enhanced ROC plotting functions with actual model results\n",
    "\n",
    "    Parameters:\n",
    "    - model_results_dict: Dictionary containing model results\n",
    "    - test_probabilities: List of probability arrays from each model\n",
    "    - y_test: True test labels\n",
    "    - class_names: List of class names\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating Enhanced ROC Visualizations...\")\n",
    "\n",
    "    # Extract model names from results\n",
    "    model_names = list(model_results_dict.keys())\n",
    "\n",
    "    if len(test_probabilities) > 0 and len(model_names) > 0:\n",
    "        # Use the original enhanced ROC function\n",
    "        print(\"Plotting individual model ROC curves with seaborn styling...\")\n",
    "        plot_roc_curves(test_probabilities, y_test, class_names, model_names)\n",
    "\n",
    "        # Use the comprehensive ROC dashboard\n",
    "        print(\"Creating comprehensive ROC comparison dashboard...\")\n",
    "        auc_scores = plot_enhanced_roc_comparison(test_probabilities, y_test, class_names, model_names)\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\nEnhanced ROC Analysis Complete!\")\n",
    "        print(\"Generated files:\")\n",
    "        print(\"   • enhanced_roc_curves_seaborn.png\")\n",
    "        print(\"   • enhanced_roc_dashboard_seaborn.png\")\n",
    "\n",
    "        return auc_scores\n",
    "    else:\n",
    "        print(\"No model probabilities available for ROC plotting\")\n",
    "        return None\n",
    "\n",
    "# Global variables to store model probabilities for ROC plotting\n",
    "MODEL_PROBABILITIES = []  # Will store probability outputs from each model\n",
    "MODEL_RESULTS_GLOBAL = {}  # Will store all model results\n",
    "TEST_LABELS_GLOBAL = None  # Will store test labels\n",
    "\n",
    "def store_model_probabilities(model_name, probabilities, test_labels=None):\n",
    "    \"\"\"\n",
    "    Store model probabilities for later ROC plotting\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: Name of the model\n",
    "    - probabilities: Model probability predictions\n",
    "    - test_labels: Test labels (only needed once)\n",
    "    \"\"\"\n",
    "    global MODEL_PROBABILITIES, TEST_LABELS_GLOBAL\n",
    "\n",
    "    MODEL_PROBABILITIES.append(probabilities)\n",
    "\n",
    "    if test_labels is not None:\n",
    "        TEST_LABELS_GLOBAL = test_labels\n",
    "\n",
    "    print(f\"Stored probabilities for {model_name} (Total models: {len(MODEL_PROBABILITIES)})\")\n",
    "\n",
    "def plot_all_model_roc_curves():\n",
    "    \"\"\"\n",
    "    Plot ROC curves for all stored models\n",
    "    \"\"\"\n",
    "    global MODEL_PROBABILITIES, MODEL_RESULTS_GLOBAL, TEST_LABELS_GLOBAL\n",
    "\n",
    "    if len(MODEL_PROBABILITIES) > 0 and TEST_LABELS_GLOBAL is not None:\n",
    "        model_names = list(MODEL_RESULTS_GLOBAL.keys())\n",
    "        class_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']  # Update based on your classes\n",
    "\n",
    "        print(f\"\\nPlotting ROC curves for {len(model_names)} models...\")\n",
    "\n",
    "        # Use both plotting functions\n",
    "        auc_scores = demonstrate_enhanced_roc_plotting(\n",
    "            MODEL_RESULTS_GLOBAL,\n",
    "            MODEL_PROBABILITIES,\n",
    "            TEST_LABELS_GLOBAL,\n",
    "            class_names\n",
    "        )\n",
    "\n",
    "        return auc_scores\n",
    "    else:\n",
    "        print(\"No model data available for ROC plotting. Train some models first!\")\n",
    "        return None\n",
    "\n",
    "print(\"Enhanced ROC plotting utilities loaded!\")\n",
    "print(\"Usage:\")\n",
    "print(\"   1. Use store_model_probabilities(model_name, probabilities, test_labels) after each model training\")\n",
    "print(\"   2. Use plot_all_model_roc_curves() to generate comprehensive ROC analysis\")\n",
    "print(\"   3. Or use demonstrate_enhanced_roc_plotting() directly with your data\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTsAEGfKFr_e"
   },
   "source": [
    "# Enhanced Optimization System"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bke5N1MvFr_e",
    "outputId": "30809d6b-a5a5-49e3-81ad-c36bb78f10cc"
   },
   "source": [
    "# Note: LabelSmoothingCrossEntropy, FocalLoss, and other utility classes are now defined in section 3.1\n",
    "\n",
    "def enhance_model_architecture(model, model_type, num_classes, dropout_rate=0.3):\n",
    "    \"\"\"Enhanced model architecture with regularization\"\"\"\n",
    "    if model_type == \"resnet\":\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.fc.in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    elif model_type == \"mobilenet\":\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.last_channel, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    elif model_type == \"efficientnet\":\n",
    "        model._fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model._fc.in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    return model\n",
    "\n",
    "def train_model_fine_tuned(model, train_loader, val_loader, criterion, epochs, device, model_name):\n",
    "    \"\"\"Enhanced training with 8 optimization techniques\"\"\"\n",
    "    print(f\"Enhanced Training for {model_name} with 8 Advanced Optimizations\")\n",
    "\n",
    "    # 1. AdamW Optimizer (better weight decay)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01, eps=1e-8)\n",
    "\n",
    "    # 2. Cosine Annealing LR Scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # 3. Label Smoothing\n",
    "    criterion_smooth = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "    # 4. Mixed Precision Training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed Precision Forward Pass\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion_smooth(outputs, labels)\n",
    "\n",
    "            # Mixed Precision Backward Pass\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 6. Gradient Clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # print(\"done\")\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        # print(\"pt 2\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion_smooth(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Learning Rate Scheduling\n",
    "        scheduler.step()\n",
    "\n",
    "        # Progress reporting\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "    print(f\"Enhanced training completed for {model_name}!\")\n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "print(\"Enhanced optimization system loaded successfully!\")\n",
    "print(\"Available optimizations:\")\n",
    "print(\"   1. AdamW optimizer with proper weight decay\")\n",
    "print(\"   2. Cosine Annealing LR scheduler\")\n",
    "print(\"   3. Label Smoothing (0.1)\")\n",
    "print(\"   4. Mixed Precision Training (AMP)\")\n",
    "print(\"   5. Gradient Clipping (max_norm=1.0)\")\n",
    "print(\"   6. Enhanced model architectures\")\n",
    "print(\"   7. Advanced regularization techniques\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a-9pfWXFr_f",
    "outputId": "724aee42-dfa5-4f92-eea7-12b8718d2cf1"
   },
   "source": [
    "\"\"\"\n",
    "Enhanced Training Data CSV Export Integration\n",
    "============================================\n",
    "\n",
    "Add this code to your Jupyter notebook to enable automatic CSV saving\n",
    "of training metrics (training accuracy, validation accuracy, training loss,\n",
    "validation loss) per epoch for each model.\n",
    "\n",
    "Usage:\n",
    "1. Run this cell in your notebook\n",
    "2. The training function will automatically save CSV files\n",
    "3. Use the plotting functions to visualize results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def save_training_data_to_csv(model_name, train_losses, train_accs, val_losses, val_accs):\n",
    "    \"\"\"Save training metrics to CSV file for each model\"\"\"\n",
    "    results_dir = 'thermal_analysis_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'epoch': epochs_range,\n",
    "        'training_loss': train_losses,\n",
    "        'training_accuracy': train_accs,\n",
    "        'validation_loss': val_losses,\n",
    "        'validation_accuracy': val_accs,\n",
    "        'model_name': model_name,\n",
    "        'timestamp': timestamp\n",
    "    })\n",
    "\n",
    "    filename = f'{model_name.lower().replace(\" \", \"_\")}_training_curves_{timestamp}.csv'\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"Training data saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def plot_training_curves_from_csv(csv_file_path):\n",
    "    \"\"\"Plot training curves from saved CSV data\"\"\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    model_name = df['model_name'].iloc[0]\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Training and Validation Loss\n",
    "    ax1.plot(df['epoch'], df['training_loss'], 'o-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(df['epoch'], df['validation_loss'], 's-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title(f'{model_name} - Training & Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Training and Validation Accuracy\n",
    "    ax2.plot(df['epoch'], df['training_accuracy'], 'o-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(df['epoch'], df['validation_accuracy'], 's-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title(f'{model_name} - Training & Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Overfitting Analysis\n",
    "    loss_diff = df['validation_loss'] - df['training_loss']\n",
    "    ax3.plot(df['epoch'], loss_diff, 'o-', color='red', linewidth=2)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax3.set_title(f'{model_name} - Overfitting Analysis (Val - Train Loss)')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss Difference')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy Gap\n",
    "    acc_diff = df['training_accuracy'] - df['validation_accuracy']\n",
    "    ax4.plot(df['epoch'], acc_diff, 'o-', color='orange', linewidth=2)\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_title(f'{model_name} - Accuracy Gap (Train - Val)')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Accuracy Difference')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_all_models_comparison():\n",
    "    \"\"\"Plot comparison of all models from CSV files\"\"\"\n",
    "    results_dir = 'thermal_analysis_results'\n",
    "    csv_files = glob.glob(os.path.join(results_dir, '*_training_curves_*.csv'))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No training curve CSV files found\")\n",
    "        return\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    colors = plt.cm.Set1(range(len(csv_files)))\n",
    "\n",
    "    for i, csv_file in enumerate(csv_files):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        model_name = df['model_name'].iloc[0]\n",
    "        color = colors[i]\n",
    "\n",
    "        # Plot all metrics\n",
    "        ax1.plot(df['epoch'], df['training_loss'], 'o-', label=model_name,\n",
    "                color=color, linewidth=2, markersize=4)\n",
    "        ax2.plot(df['epoch'], df['validation_loss'], 's-', label=model_name,\n",
    "                color=color, linewidth=2, markersize=4)\n",
    "        ax3.plot(df['epoch'], df['training_accuracy'], 'o-', label=model_name,\n",
    "                color=color, linewidth=2, markersize=4)\n",
    "        ax4.plot(df['epoch'], df['validation_accuracy'], 's-', label=model_name,\n",
    "                color=color, linewidth=2, markersize=4)\n",
    "\n",
    "    # Configure plots\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Validation Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3.set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Training Accuracy')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4.set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Validation Accuracy')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Model Training Curves Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Enhanced training function with automatic CSV saving\n",
    "def train_model_with_csv_export(model, train_loader, val_loader, criterion, epochs, device, model_name):\n",
    "    \"\"\"\n",
    "    Enhanced training function that automatically saves training data to CSV\n",
    "\n",
    "    This function wraps the existing train_model_fine_tuned function and adds\n",
    "    automatic CSV export functionality.\n",
    "    \"\"\"\n",
    "    # Call the original training function\n",
    "    train_losses, train_accs, val_losses, val_accs = train_model_fine_tuned(\n",
    "        model, train_loader, val_loader, criterion, epochs, device, model_name\n",
    "    )\n",
    "\n",
    "    # Save data to CSV\n",
    "    save_training_data_to_csv(model_name, train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "    # Store in enhanced training data for other visualizations\n",
    "    if 'enhanced_training_data' in globals():\n",
    "        enhanced_training_data[model_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs\n",
    "        }\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "# Function to create training summary\n",
    "def create_training_summary():\n",
    "    \"\"\"Create a summary table of all training results\"\"\"\n",
    "    results_dir = 'thermal_analysis_results'\n",
    "    csv_files = glob.glob(os.path.join(results_dir, '*_training_curves_*.csv'))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found\")\n",
    "        return None\n",
    "\n",
    "    summary_data = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        model_name = df['model_name'].iloc[0]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Epochs': len(df),\n",
    "            'Final Train Acc': f\"{df['training_accuracy'].iloc[-1]:.4f}\",\n",
    "            'Final Val Acc': f\"{df['validation_accuracy'].iloc[-1]:.4f}\",\n",
    "            'Best Val Acc': f\"{df['validation_accuracy'].max():.4f}\",\n",
    "            'Final Train Loss': f\"{df['training_loss'].iloc[-1]:.4f}\",\n",
    "            'Final Val Loss': f\"{df['validation_loss'].iloc[-1]:.4f}\",\n",
    "            'Min Val Loss': f\"{df['validation_loss'].min():.4f}\"\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"Training Results Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary\n",
    "    summary_file = os.path.join(results_dir, f'training_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"\\nSummary saved to: {summary_file}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "print(\"1. Train your models (CSV files will be automatically saved)\")\n",
    "print(\"2. Use plot_training_curves_from_csv('path/to/file.csv') for individual plots\")\n",
    "print(\"3. Use plot_all_models_comparison() to compare all models\")\n",
    "print(\"4. Use create_training_summary() to generate summary table\")\n",
    "print(\"\\nAll files will be saved in 'thermal_analysis_results' directory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models\n",
    "\n",
    "All model architectures are defined in this section for better organization and reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label Smoothing Cross Entropy Loss for better generalization\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_class = pred.size(1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - self.smoothing) + (1 - one_hot) * self.smoothing / (n_class - 1)\n",
    "        log_prob = F.log_softmax(pred, dim=1)\n",
    "        return torch.mean(torch.sum(-one_hot * log_prob, dim=1))\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined loss function using both CrossEntropy and Focal Loss\"\"\"\n",
    "    def __init__(self, alpha=0.6, beta=0.4):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.focal_loss = FocalLoss(alpha=1, gamma=2)\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        ce = self.ce_loss(predictions, targets)\n",
    "        focal = self.focal_loss(predictions, targets)\n",
    "        return self.alpha * ce + self.beta * focal\n",
    "\n",
    "print(\"Utility loss functions loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism for feature enhancement\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        attention = self.sigmoid(self.conv(x_cat))\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention mechanism for feature recalibration\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y1 = self.avg_pool(x).view(b, c)\n",
    "        y2 = self.max_pool(x).view(b, c)\n",
    "\n",
    "        y1 = self.fc(y1)\n",
    "        y2 = self.fc(y2)\n",
    "\n",
    "        attention = self.sigmoid(y1 + y2).view(b, c, 1, 1)\n",
    "        return x * attention.expand_as(x)\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "print(\"Attention mechanisms loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 VGG16 Thermal Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VGG16Thermal(nn.Module):\n",
    "    \"\"\"VGG16 model adapted for thermal image classification\"\"\"\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(VGG16Thermal, self).__init__()\n",
    "        self.backbone = tv_models.vgg16(pretrained=True)\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print(\"VGG16Thermal model loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 AlexNet Thermal Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class AlexNetThermal(nn.Module):\n",
    "    \"\"\"Enhanced AlexNet for thermal imaging with improved architecture\"\"\"\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(AlexNetThermal, self).__init__()\n",
    "        \n",
    "        # Load pretrained AlexNet\n",
    "        self.alexnet = tv_models.alexnet(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolution layer to accept thermal images\n",
    "        # Keep the pretrained weights for RGB channels\n",
    "        original_conv1 = self.alexnet.features[0]\n",
    "        self.alexnet.features[0] = nn.Conv2d(\n",
    "            3, 64, kernel_size=11, stride=4, padding=2\n",
    "        )\n",
    "        \n",
    "        # Copy pretrained weights\n",
    "        with torch.no_grad():\n",
    "            self.alexnet.features[0].weight.copy_(original_conv1.weight)\n",
    "            if self.alexnet.features[0].bias is not None:\n",
    "                self.alexnet.features[0].bias.copy_(original_conv1.bias)\n",
    "        \n",
    "        # Replace classifier with improved architecture\n",
    "        self.alexnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights for the custom classifier\"\"\"\n",
    "        for m in self.alexnet.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle both 1-channel thermal and 3-channel RGB inputs\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        elif x.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected 1 or 3 input channels, got {x.shape[1]}\")\n",
    "        return self.alexnet(x)\n",
    "\n",
    "print(\"AlexNetThermal model loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Enhanced Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class EnhancedHybridVGGAlexNet(nn.Module):\n",
    "    \"\"\"Enhanced Hybrid model combining VGG16 and AlexNet with attention mechanisms\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
    "        super(EnhancedHybridVGGAlexNet, self).__init__()\n",
    "        \n",
    "        # Load pre-trained backbones\n",
    "        vgg16 = tv_models.vgg16(pretrained=True)\n",
    "        alexnet = tv_models.alexnet(pretrained=True)\n",
    "        \n",
    "        # VGG16 feature extraction\n",
    "        self.vgg_features = nn.Sequential(*list(vgg16.features[:-1]))\n",
    "        \n",
    "        # AlexNet feature extraction\n",
    "        self.alex_features = nn.Sequential(*list(alexnet.features))\n",
    "        \n",
    "        # Attention mechanisms\n",
    "        self.vgg_attention = CBAM(512)\n",
    "        self.alex_attention = CBAM(256)\n",
    "        \n",
    "        # Feature adaptation layers\n",
    "        self.vgg_adapt = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        \n",
    "        self.alex_adapt = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        \n",
    "        # Enhanced fusion layers\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            nn.Conv2d(1024, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # Enhanced classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.7),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize custom layer weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # VGG16 pathway\n",
    "        vgg_features = self.vgg_features(x)\n",
    "        vgg_features = self.vgg_attention(vgg_features)\n",
    "        vgg_features = self.vgg_adapt(vgg_features)\n",
    "        \n",
    "        # AlexNet pathway\n",
    "        alex_features = self.alex_features(x)\n",
    "        alex_features = self.alex_attention(alex_features)\n",
    "        alex_features = self.alex_adapt(alex_features)\n",
    "        \n",
    "        # Align spatial dimensions\n",
    "        alex_features = F.interpolate(alex_features, size=vgg_features.shape[-2:],\n",
    "                                    mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Feature fusion\n",
    "        fused_features = torch.cat([vgg_features, alex_features], dim=1)\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        \n",
    "        # Classification\n",
    "        features = fused_features.flatten(1)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"EnhancedHybridVGGAlexNet model loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Custom CNN with Inception Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    \"\"\"Inception-style module for multi-scale feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, filters, name_prefix=\"inception\"):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        # Branch 1: 1x1 convolution\n",
    "        self.branch1 = nn.Conv2d(in_channels, filters//4, kernel_size=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(filters//4)\n",
    "        \n",
    "        # Branch 2: 1x1 -> 3x3 convolution\n",
    "        self.branch2_reduce = nn.Conv2d(in_channels, filters//8, kernel_size=1, padding=0)\n",
    "        self.bn2_reduce = nn.BatchNorm2d(filters//8)\n",
    "        self.branch2_conv = nn.Conv2d(filters//8, filters//4, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(filters//4)\n",
    "        \n",
    "        # Branch 3: 1x1 -> 3x3 -> 3x3 (simulating 5x5)\n",
    "        self.branch3_reduce = nn.Conv2d(in_channels, filters//16, kernel_size=1, padding=0)\n",
    "        self.bn3_reduce = nn.BatchNorm2d(filters//16)\n",
    "        self.branch3_conv1 = nn.Conv2d(filters//16, filters//8, kernel_size=3, padding=1)\n",
    "        self.bn3_conv1 = nn.BatchNorm2d(filters//8)\n",
    "        self.branch3_conv2 = nn.Conv2d(filters//8, filters//4, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(filters//4)\n",
    "        \n",
    "        # Branch 4: MaxPooling -> 1x1\n",
    "        self.branch4_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.branch4_conv = nn.Conv2d(in_channels, filters//4, kernel_size=1, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(filters//4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Branch 1\n",
    "        branch1 = F.relu(self.bn1(self.branch1(x)))\n",
    "        \n",
    "        # Branch 2\n",
    "        branch2 = F.relu(self.bn2_reduce(self.branch2_reduce(x)))\n",
    "        branch2 = F.relu(self.bn2(self.branch2_conv(branch2)))\n",
    "        \n",
    "        # Branch 3\n",
    "        branch3 = F.relu(self.bn3_reduce(self.branch3_reduce(x)))\n",
    "        branch3 = F.relu(self.bn3_conv1(self.branch3_conv1(branch3)))\n",
    "        branch3 = F.relu(self.bn3(self.branch3_conv2(branch3)))\n",
    "        \n",
    "        # Branch 4\n",
    "        branch4 = self.branch4_pool(x)\n",
    "        branch4 = F.relu(self.bn4(self.branch4_conv(branch4)))\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ProposedCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture with Inception modules for thermal imaging\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(3, 200, 200), num_classes=5, dropout_rate=0.3):\n",
    "        super(ProposedCNN, self).__init__()\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Second convolution block\n",
    "        self.conv2a = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Inception modules\n",
    "        self.inception3a = InceptionModule(128, 256, \"inception3a\")\n",
    "        self.inception3b = InceptionModule(256, 256, \"inception3b\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.inception4a = InceptionModule(256, 512, \"inception4a\")\n",
    "        self.inception4b = InceptionModule(512, 512, \"inception4b\")\n",
    "        self.inception4c = InceptionModule(512, 512, \"inception4c\")\n",
    "        self.inception4d = InceptionModule(512, 512, \"inception4d\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.inception5a = InceptionModule(512, 1024, \"inception5a\")\n",
    "        self.inception5b = InceptionModule(1024, 1024, \"inception5b\")\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Deep convolution layers\n",
    "        self.conv_deep1 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        self.bn_deep1 = nn.BatchNorm2d(1024)\n",
    "        self.conv_deep2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        self.bn_deep2 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.dropout6 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout7 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layer\n",
    "        self.predictions = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Second convolution block\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Inception modules\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Deep convolution layers\n",
    "        x = F.relu(self.bn_deep1(self.conv_deep1(x)))\n",
    "        x = F.relu(self.bn_deep2(self.conv_deep2(x)))\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout6(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout7(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.predictions(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"ProposedCNN and InceptionModule models loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz2d63zwFr_f"
   },
   "source": [
    "# Data Augmentation & Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDygEkhhFr_f",
    "outputId": "3d8de42b-6375-4738-8dc1-e552a76a4fab"
   },
   "source": [
    "def save_augmented_images(X, y, save_dir, categories):\n",
    "    \"\"\"Save augmented images to disk organized by class\"\"\"\n",
    "    print(f\"Saving augmented images to {save_dir}...\")\n",
    "\n",
    "    # Create main directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Create subdirectories for each class\n",
    "    for category in categories:\n",
    "        class_dir = os.path.join(save_dir, category)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    # Save images by class\n",
    "    class_counts = {}\n",
    "    for i, category in enumerate(categories):\n",
    "        class_counts[i] = 0\n",
    "\n",
    "    for idx, (img, label) in enumerate(zip(X, y)):\n",
    "        category_name = categories[label]\n",
    "        class_counts[label] += 1\n",
    "\n",
    "        # Convert numpy array to PIL Image and save\n",
    "        img_uint8 = (img * 255).astype(np.uint8)\n",
    "        pil_img = PILImage.fromarray(img_uint8, mode='RGB')\n",
    "\n",
    "        filename = f\"{class_counts[label]:05d}.png\"\n",
    "        filepath = os.path.join(save_dir, category_name, filename)\n",
    "        pil_img.save(filepath)\n",
    "\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"   Saved {idx + 1}/{len(X)} images...\")\n",
    "\n",
    "    print(f\"Successfully saved {len(X)} images\")\n",
    "    for i, category in enumerate(categories):\n",
    "        print(f\"   {category}: {class_counts[i]} images\")\n",
    "\n",
    "\n",
    "def load_augmented_images(load_dir, categories):\n",
    "    \"\"\"Load augmented images from disk\"\"\"\n",
    "    print(f\" Loading augmented images from {load_dir}...\")\n",
    "\n",
    "    images, labels = [], []\n",
    "    label_map = {cat: i for i, cat in enumerate(categories)}\n",
    "\n",
    "    for category in categories:\n",
    "        class_dir = os.path.join(load_dir, category)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"    Directory not found: {class_dir}\")\n",
    "            continue\n",
    "\n",
    "        image_files = sorted([f for f in os.listdir(class_dir) if f.endswith('.png')])\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            try:\n",
    "                with PILImage.open(img_path) as im:\n",
    "                    img = im.convert(\"RGB\")\n",
    "                    arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "                    images.append(arr)\n",
    "                labels.append(label_map[category])\n",
    "            except Exception as e:\n",
    "                print(f\"   Error loading {img_path}: {e}\")\n",
    "\n",
    "        if len(image_files) > 0:\n",
    "            print(f\"   {category}: {len(image_files)} images loaded\")\n",
    "\n",
    "    if len(images) > 0:\n",
    "        print(f\"Successfully loaded {len(images)} images\")\n",
    "        return np.array(images, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def check_augmented_images_exist(load_dir, categories, expected_count_per_class):\n",
    "    \"\"\"Check if augmented images already exist with expected counts\"\"\"\n",
    "    if not os.path.exists(load_dir):\n",
    "        return False\n",
    "\n",
    "    for category in categories:\n",
    "        class_dir = os.path.join(load_dir, category)\n",
    "        if not os.path.exists(class_dir):\n",
    "            return False\n",
    "\n",
    "        image_files = [f for f in os.listdir(class_dir) if f.endswith('.png')]\n",
    "        if len(image_files) < expected_count_per_class:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def augment_with_target_count(X, y, target_count_per_class=1000, batch_size=50):\n",
    "    \"\"\"Enhanced data augmentation with target count control\"\"\"\n",
    "    print(f\"Target count per class: {target_count_per_class}\")\n",
    "    print(f\"Original class distribution: {Counter(y)}\")\n",
    "\n",
    "    H = W = 200\n",
    "    pad_px = 24\n",
    "    augment_tfms = transforms.Compose([\n",
    "        transforms.Pad(pad_px, padding_mode='reflect'),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=35,\n",
    "            translate=(0.25, 0.25),\n",
    "            scale=(0.85, 1.15),\n",
    "            shear=25,\n",
    "            interpolation=InterpolationMode.BILINEAR,\n",
    "            fill=0\n",
    "        ),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=(0.9, 1.1),\n",
    "            contrast=(0.95, 1.05),\n",
    "            saturation=(0.95, 1.05)\n",
    "        ),\n",
    "        transforms.CenterCrop((H, W)),\n",
    "    ])\n",
    "\n",
    "    X_augmented, y_augmented = [], []\n",
    "    unique_classes = np.unique(y)\n",
    "\n",
    "    for class_idx in unique_classes:\n",
    "        class_mask = y == class_idx\n",
    "        class_data = X[class_mask]\n",
    "        current_count = len(class_data)\n",
    "        print(f\"Processing class {class_idx}: {current_count} samples\")\n",
    "\n",
    "        # Add original data first\n",
    "        X_augmented.extend(class_data)\n",
    "        y_augmented.extend([class_idx] * current_count)\n",
    "\n",
    "        if current_count < target_count_per_class:\n",
    "            needed = target_count_per_class - current_count\n",
    "            print(f\"    Augmenting {needed} additional samples\")\n",
    "            generated = 0\n",
    "            while generated < needed:\n",
    "                batch_needed = min(batch_size, needed - generated)\n",
    "                batch_X = []\n",
    "\n",
    "                for _ in range(batch_needed):\n",
    "                    idx = np.random.randint(0, len(class_data))\n",
    "                    img = class_data[idx]\n",
    "                    img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "                    aug_tensor = augment_tfms(img_tensor)\n",
    "                    aug_img = aug_tensor.permute(1, 2, 0).numpy()\n",
    "                    aug_img = np.clip(aug_img, 0, 1)\n",
    "                    batch_X.append(aug_img)\n",
    "\n",
    "                X_augmented.extend(batch_X)\n",
    "                y_augmented.extend([class_idx] * batch_needed)\n",
    "                generated += batch_needed\n",
    "\n",
    "                if generated % 100 == 0 or generated == needed:\n",
    "                    print(f\"      Generated {generated}/{needed} samples\")\n",
    "\n",
    "                del batch_X\n",
    "                gc.collect()\n",
    "        else:\n",
    "            print(f\"   Class has sufficient samples ({current_count} >= {target_count_per_class})\")\n",
    "\n",
    "    print(f\"Augmentation complete!\")\n",
    "    print(f\"Final dataset size: {len(X_augmented)} samples\")\n",
    "\n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "\n",
    "# Configuration\n",
    "TARGET_COUNT = 4000\n",
    "\n",
    "# Load original data to get categories\n",
    "print(\" Loading thermal imaging data...\")\n",
    "images, labels = [], []\n",
    "categories = sorted([d for d in os.listdir(data_dir)\n",
    "                     if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# Check if augmented images already exist\n",
    "if check_augmented_images_exist(AUGMENTED_IMAGES_DIR, categories, TARGET_COUNT):\n",
    "    print(\"\\nFound existing augmented images! Loading from disk...\")\n",
    "    X_augmented, y_augmented = load_augmented_images(AUGMENTED_IMAGES_DIR, categories)\n",
    "\n",
    "    if X_augmented is not None and y_augmented is not None:\n",
    "        print(f\"\\nLoaded Dataset Statistics:\")\n",
    "        print(f\"Total samples: {len(X_augmented)}\")\n",
    "        for i, category in enumerate(categories):\n",
    "            count = np.sum(y_augmented == i)\n",
    "            percentage = (count / len(y_augmented)) * 100\n",
    "            print(f\"  {category}: {count} samples ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\" Failed to load augmented images. Will generate new ones.\")\n",
    "        X_augmented = None\n",
    "else:\n",
    "    print(\"\\n Augmented images not found or incomplete. Will generate new ones...\")\n",
    "    X_augmented = None\n",
    "\n",
    "# Generate and save augmented images if not loaded\n",
    "if X_augmented is None:\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        for image_name in os.listdir(category_path):\n",
    "            if image_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                image_path = os.path.join(category_path, image_name)\n",
    "                try:\n",
    "                    with PILImage.open(image_path) as im:\n",
    "                        img = im.convert(\"RGB\").resize((200, 200))\n",
    "                        arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "                        images.append(arr)\n",
    "                    labels.append(category)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    labels = np.array(labels)\n",
    "    label_map = {cat: i for i, cat in enumerate(categories)}\n",
    "    numerical_labels = np.array([label_map[label] for label in labels], dtype=np.int64)\n",
    "\n",
    "    X, y = images, numerical_labels\n",
    "\n",
    "    print(f\"\\nOriginal Dataset Statistics:\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    for i, category in enumerate(categories):\n",
    "        count = np.sum(y == i)\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"  {category}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    print(\"\\nApplying enhanced data augmentation...\")\n",
    "    X_augmented, y_augmented = augment_with_target_count(\n",
    "        X, y, target_count_per_class=TARGET_COUNT, batch_size=50\n",
    "    )\n",
    "\n",
    "    print(\"\\nSaving augmented images for future use...\")\n",
    "    save_augmented_images(X_augmented, y_augmented, AUGMENTED_IMAGES_DIR, categories)\n",
    "\n",
    "# Enhanced train/validation/test split (70/15/15)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_augmented, y_augmented, test_size=0.3, random_state=42, stratify=y_augmented\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData split completed:\")\n",
    "print(f\"Training samples: {len(X_train)} (70%)\")\n",
    "print(f\"Validation samples: {len(X_val)} (15%)\")\n",
    "print(f\"Testing samples: {len(X_test)} (15%)\")\n",
    "\n",
    "\n",
    "def prepare_enhanced_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Prepare enhanced DataLoaders with train/validation/test splits\"\"\"\n",
    "\n",
    "    def prepare_tensors(X, y):\n",
    "        if X.max() > 1.0:\n",
    "            X = X.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            X = X.astype(np.float32)\n",
    "        X_tensor = torch.from_numpy(X).permute(0, 3, 1, 2)\n",
    "        y_tensor = torch.from_numpy(y.astype(np.int64))\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "    X_train_tensor, y_train_tensor = prepare_tensors(X_train, y_train)\n",
    "    X_val_tensor, y_val_tensor = prepare_tensors(X_val, y_val)\n",
    "    X_test_tensor, y_test_tensor = prepare_tensors(X_test, y_test)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader_enhanced, val_loader_enhanced, test_loader_enhanced = prepare_enhanced_dataloaders(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n Enhanced DataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader_enhanced)}\")\n",
    "print(f\"Validation batches: {len(val_loader_enhanced)}\")\n",
    "print(f\"Test batches: {len(test_loader_enhanced)}\")\n",
    "\n",
    "# Global storage for model results\n",
    "model_results = []\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN1acFxEFr_f"
   },
   "source": "# Evaluation Functions"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_oqNJ5-Fr_f",
    "outputId": "daf81121-62e1-4dcd-8696-cde5a218de60"
   },
   "source": [
    "def evaluate_model_comprehensive(model, test_loader, class_names, model_name, device):\n",
    "    \"\"\"Comprehensive evaluation with all metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision_macro = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    precision_weighted = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall_macro = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    recall_weighted = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate micro-averaged metrics\n",
    "    precision_micro = precision_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_predictions, average='micro', zero_division=0)\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{model_name} - Test Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision (Macro): {precision_macro:.4f}\")\n",
    "    print(f\"  Precision (Weighted): {precision_weighted:.4f}\")\n",
    "    print(f\"  Recall (Macro): {recall_macro:.4f}\")\n",
    "    print(f\"  Recall (Weighted): {recall_weighted:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(f\"{model_name} - Detailed Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions,\n",
    "                              target_names=class_names, zero_division=0))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 7), dpi=300)\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=class_names, yticklabels=class_names,\n",
    "        linewidths=0.5, linecolor='white',\n",
    "        cbar=True, cbar_kws={'label': None},\n",
    "        annot_kws={\"size\": 19}  # <-- Bigger, bold numbers\n",
    "    )\n",
    "\n",
    "    # Axis labels\n",
    "    plt.xlabel('Predicted', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Actual', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Ticks formatting\n",
    "    plt.xticks(fontsize=14, rotation=0)     # Horizontal x-axis ticks\n",
    "    plt.yticks(fontsize=14, rotation=90)    # Vertical y-axis ticks\n",
    "\n",
    "    # Layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_macro': recall_macro,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_training_curves(train_losses, train_accs, val_losses, val_accs, model_name):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "    ax1.plot(val_losses, label='Validation Loss', color='red')\n",
    "    ax1.set_title(f'{model_name} - Training & Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
    "    ax2.plot(val_accs, label='Validation Accuracy', color='red')\n",
    "    ax2.set_title(f'{model_name} - Training & Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Enhanced evaluation functions loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVWCOL1QFr_f"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MosHzwMKFr_f"
   },
   "source": [
    "## ResNet Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "id": "qPL4EuK0Fr_f",
    "outputId": "7cba4a79-2168-4c61-c1d4-48ef6f40ca3a"
   },
   "source": [
    "\n",
    "# Enhanced ResNet Training and Evaluation\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\" ENHANCED RESNET18 TRAINING WITH VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_classes = len(categories)\n",
    "resnet_enhanced = models.resnet18(pretrained=False)\n",
    "\n",
    "# Enhance architecture with regularization\n",
    "resnet_enhanced = enhance_model_architecture(resnet_enhanced, \"resnet\", num_classes, dropout_rate=0.3)\n",
    "# CUDA memory management and error handling\n",
    "try:\n",
    "    # Clear CUDA cache before model transfer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    resnet_enhanced = resnet_enhanced.to(device)\n",
    "    print(f\"ResNet model successfully moved to {device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e) or \"out of memory\" in str(e):\n",
    "        print(f\"CUDA error encountered: {e}\")\n",
    "        print(\"Falling back to CPU...\")\n",
    "        device = torch.device('cpu')\n",
    "        resnet_enhanced = resnet_enhanced.to(device)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train with validation\n",
    "if resnet_model_training:\n",
    "  epochs = common_epochs\n",
    "  train_losses, train_accs, val_losses, val_accs = train_model_fine_tuned(\n",
    "      resnet_enhanced, train_loader_enhanced, val_loader_enhanced,\n",
    "      criterion, epochs, device, \"ResNet18\")\n",
    "\n",
    "  # Plot training curves\n",
    "  plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"ResNet18\")\n",
    "  print (\"Training is completed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save Model"
   ],
   "metadata": {
    "id": "Df8GClcbEm8D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for ResNet18\n",
    "model_name = \"resnet18_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': resnet_enhanced.state_dict(),\n",
    "    'model_architecture': 'ResNet18',\n",
    "    'model_class': 'resnet18',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.3,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'CosineAnnealingLR',\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(resnet_enhanced, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"ResNet18 Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kpHLXIHEmHD",
    "outputId": "d3c1d8ca-b399-4914-d158-4703a383554f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": " ### Evaluation",
   "metadata": {
    "id": "5v3tPCImFHT6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"resnet18_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "resnet_enhanced = torch.load(model_path, map_location=device, weights_only=False)\n",
    "resnet_enhanced.eval()\n",
    "print(\"Full Resnet model loaded successfully\")\n",
    "\n",
    "\n",
    "# Comprehensive evaluation\n",
    "resnet_results = evaluate_model_comprehensive(resnet_enhanced, test_loader_enhanced, categories, \"ResNet18\", device)\n",
    "save_training_data_to_csv(\"ResNet18\", train_losses, train_accs, val_losses, val_accs)\n",
    "# Store model probabilities for ROC plotting\n",
    "with torch.no_grad():\n",
    "    resnet_enhanced.eval()\n",
    "    test_probs = []\n",
    "    test_labels = []\n",
    "\n",
    "    for batch_x, batch_y in test_loader_enhanced:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        outputs = resnet_enhanced(batch_x)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_labels_array = np.hstack(test_labels)\n",
    "\n",
    "# Store for ROC plotting\n",
    "store_model_probabilities('ResNet18', test_probs, test_labels_array)\n",
    "MODEL_RESULTS_GLOBAL['ResNet18'] = resnet_results\n",
    "\n",
    "\n",
    "# Store results\n",
    "model_results.append(resnet_results)\n",
    "print(f\"ResNet18 enhanced training and evaluation completed!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dim49Cl3FGf7",
    "outputId": "ee3a4bb5-41fa-4222-e54e-25b01e61afa3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZgD4D1sFr_f"
   },
   "source": [
    "## MobileNet Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "irs2eebrFr_f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "outputId": "6d287895-df25-461c-c940-3bb5f2b2e5cb"
   },
   "source": [
    "# Enhanced MobileNet Training and Evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ENHANCED MOBILENETV2 TRAINING WITH VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_classes = len(categories)\n",
    "mobilenet_enhanced = models.mobilenet_v2(pretrained=False)\n",
    "\n",
    "# Enhance architecture with regularization\n",
    "mobilenet_enhanced = enhance_model_architecture(mobilenet_enhanced, \"mobilenet\", num_classes, dropout_rate=0.3)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train with validation\n",
    "#epochs = 5\n",
    "if mobilenet_model_training:\n",
    "  epochs = common_epochs\n",
    "  train_losses, train_accs, val_losses, val_accs = train_model_fine_tuned(\n",
    "      mobilenet_enhanced, train_loader_enhanced, val_loader_enhanced,\n",
    "      criterion, epochs, device, \"MobileNetV2\")\n",
    "\n",
    "  # Plot training curves\n",
    "  plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"MobileNetV2\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "id": "sUV8nUMhH2iQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for MobileNetV2\n",
    "model_name = \"mobilenet_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': mobilenet_enhanced.state_dict(),\n",
    "    'model_architecture': 'MobileNetV2',\n",
    "    'model_class': 'mobilenet_v2',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.3,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'CosineAnnealingLR',\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(mobilenet_enhanced, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"MobileNetV2 Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_8cvTqoH2SI",
    "outputId": "61c8fe0f-69d2-482d-a5ba-99a0bd478b62"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "id": "W2PHFq0PH9RQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"mobilenet_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "mobilenet_enhanced = torch.load(model_path, map_location=device, weights_only=False)\n",
    "mobilenet_enhanced.eval()\n",
    "print(\"Full mobilenet  model loaded successfully\")\n",
    "\n",
    "\n",
    "# Comprehensive evaluation\n",
    "mobilenet_results = evaluate_model_comprehensive(mobilenet_enhanced, test_loader_enhanced, categories, \"MobileNetV2\", device)\n",
    "\n",
    "# Store model probabilities for ROC plotting\n",
    "with torch.no_grad():\n",
    "    mobilenet_enhanced.eval()\n",
    "    test_probs = []\n",
    "    test_labels = []\n",
    "    for batch_x, batch_y in test_loader_enhanced:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = mobilenet_enhanced(batch_x)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_labels_array = np.hstack(test_labels)\n",
    "\n",
    "# Store for ROC plotting\n",
    "store_model_probabilities('MobileNetV2', test_probs, test_labels_array)\n",
    "MODEL_RESULTS_GLOBAL['MobileNetV2'] = mobilenet_results\n",
    "save_training_data_to_csv(\"MobileNetV2\", train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "# Store results\n",
    "model_results.append(mobilenet_results)\n",
    "print(f\"\\nMobileNetV2 enhanced training and evaluation completed!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vWc27RjrIAQX",
    "outputId": "14601969-215d-42c8-ffe0-b5163df673c4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9LjbgebFr_f"
   },
   "source": [
    "## EfficientNet Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mvdW36QGFr_f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b86ad2a7-0c8f-4631-8255-f3c00b6c673d"
   },
   "source": [
    "# Enhanced EfficientNet Training and Evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ENHANCED EFFICIENTNET TRAINING WITH VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Try to  import EfficientNet, skip if not available\n",
    "try:\n",
    "\n",
    "    num_classes = len(categories)\n",
    "    efficientnet_enhanced = EfficientNet.from_name(\"efficientnet-b0\")\n",
    "\n",
    "    # Enhance architecture with regularization\n",
    "    efficientnet_enhanced = enhance_model_architecture(efficientnet_enhanced, \"efficientnet\", num_classes, dropout_rate=0.3)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train with validation\n",
    "    if efficientnet_model_training:\n",
    "      epochs = common_epochs\n",
    "      print(f\"Training EfficientNet with full enhanced optimization suite...\")\n",
    "\n",
    "      train_losses, train_accs, val_losses, val_accs = train_model_fine_tuned(\n",
    "          efficientnet_enhanced, train_loader_enhanced, val_loader_enhanced,\n",
    "          criterion, epochs, device, \"EfficientNet-B0\")\n",
    "\n",
    "      # Plot training curves\n",
    "      plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"EfficientNet-B0\")\n",
    "      print(f\"EfficientNet-B0 enhanced training completed!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\" EfficientNet not available: {e}\")\n",
    "    print(f\"   Install with: pip install efficientnet-pytorch\")\n",
    "    print(f\"   Continuing with ResNet and MobileNet results...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "id": "j81dAoFZLBFg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for EfficientNet-B0\n",
    "model_name = \"efficientnet_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': efficientnet_enhanced.state_dict(),\n",
    "    'model_architecture': 'EfficientNet-B0',\n",
    "    'model_class': 'efficientnet-b0',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.3,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'CosineAnnealingLR',\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(efficientnet_enhanced, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"EfficientNet-B0 Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "id": "DAH1wlgxLCWO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "id": "wbiGFfZpLI0v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"efficientnet_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "efficientnet_enhanced = torch.load(model_path, map_location=device, weights_only=False)\n",
    "efficientnet_enhanced.eval()\n",
    "print(\"Full efficientnet  model loaded successfully\")\n",
    "\n",
    "try:\n",
    "    # Comprehensive evaluation\n",
    "    efficientnet_results = evaluate_model_comprehensive(efficientnet_enhanced, test_loader_enhanced, categories, \"EfficientNet-B0\", device)\n",
    "\n",
    "\n",
    "    # Store model probabilities for ROC plotting\n",
    "    with torch.no_grad():\n",
    "        efficientnet_enhanced.eval()\n",
    "        test_probs = []\n",
    "        test_labels = []\n",
    "        for batch_x, batch_y in test_loader_enhanced:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = efficientnet_enhanced(batch_x)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            test_probs.append(probs.cpu().numpy())\n",
    "            test_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "        test_probs = np.vstack(test_probs)\n",
    "        test_labels_array = np.hstack(test_labels)\n",
    "\n",
    "    # Store for ROC plotting\n",
    "    store_model_probabilities('EfficientNet', test_probs, test_labels_array)\n",
    "    MODEL_RESULTS_GLOBAL['EfficientNet'] = efficientnet_results\n",
    "    save_training_data_to_csv(\"EfficientNet\", train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    model_results.append(efficientnet_results)\n",
    "    print(f\"EfficientNet-B0 enhanced and evaluation completed!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\" EfficientNet not available: {e}\")\n",
    "    print(f\"   Install with: pip install efficientnet-pytorch\")\n",
    "    print(f\"   Continuing with ResNet and MobileNet results...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" EfficientNet training skipped due to error: {e}\")\n",
    "    print(f\"   ResNet and MobileNet results are available for comparison\")"
   ],
   "metadata": {
    "id": "2JjwdE_bLKAW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0oMEF32Fr_g"
   },
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "INKCLAvUFr_g",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e34b6c4-0437-48af-a423-c4d317d06dea"
   },
   "source": [
    "print(\"VGG16 ENHANCED FINE-TUNING WITH ADVANCED OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "vgg16_model = VGG16Thermal(num_classes=5).to(device)\n",
    "vgg16_params = sum(p.numel() for p in vgg16_model.parameters())\n",
    "\n",
    "\n",
    "# Advanced optimizer configuration for VGG16\n",
    "vgg16_optimizer = optim.AdamW(\n",
    "    vgg16_model.parameters(),\n",
    "    lr=0.0005,  # Lower initial learning rate for fine-tuning\n",
    "    weight_decay=0.001,  # Reduced weight decay\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Multi-step learning rate scheduler for better convergence\n",
    "vgg16_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    vgg16_optimizer,\n",
    "    milestones=[10, 20, 30],\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "# Enhanced criterion with label smoothing\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_prob = F.log_softmax(pred, dim=-1)\n",
    "        weight = pred.new_ones(pred.size()) * self.smoothing / (pred.size(-1) - 1.)\n",
    "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
    "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "enhanced_criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "# Enhanced training with custom loop for better control\n",
    "def train_vgg16_enhanced(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=35):\n",
    "    model.train()\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100.0 * correct / total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model state\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'vgg16_best_enhanced.pth')\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('vgg16_best_enhanced.pth'))\n",
    "    print(f'Enhanced VGG16 training completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "\n",
    "# Execute enhanced training\n",
    "if vgg16_model_training:\n",
    "  train_losses, train_accs, val_losses, val_accs = train_vgg16_enhanced(\n",
    "      vgg16_model, train_loader_enhanced, val_loader_enhanced,\n",
    "      enhanced_criterion, vgg16_optimizer, vgg16_scheduler, epochs=common_epochs\n",
    "  )\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "id": "klQrjTysNrD9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for VGG16\n",
    "model_name = \"vgg16_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': vgg16_model.state_dict(),\n",
    "    'model_architecture': 'VGG16',\n",
    "    'model_class': 'vgg16',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.5,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'MultiStepLR',\n",
    "    'scheduler_milestones': [30, 60, 90],\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(vgg16_model, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"VGG16 Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S96gGFivNsXe",
    "outputId": "f708a62c-4ce0-4d47-dc97-f9b789b3c089"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "id": "36OZLPchNsvu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"vgg16_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "vgg16_model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "vgg16_model.eval()\n",
    "print(\"Full vgg16_epoch_100  model loaded successfully\")\n",
    "\n",
    "# Enhanced evaluation\n",
    "vgg16_results = evaluate_model_comprehensive(vgg16_model, test_loader_enhanced, categories, \"VGG16-Enhanced\", device)\n",
    "\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"VGG16\")\n",
    "save_training_data_to_csv(\"VGG16\", train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "# Store model probabilities for ROC plotting\n",
    "with torch.no_grad():\n",
    "    vgg16_model.eval()\n",
    "    test_probs = []\n",
    "    test_labels = []\n",
    "    for batch_x, batch_y in test_loader_enhanced:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = vgg16_model(batch_x)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_labels_array = np.hstack(test_labels)\n",
    "\n",
    "# Store for ROC plotting\n",
    "store_model_probabilities('VGG16', test_probs, test_labels_array)\n",
    "MODEL_RESULTS_GLOBAL['VGG16'] = vgg16_results\n",
    "\n",
    "\n",
    "\n",
    "model_results.append(vgg16_results)\n",
    "\n",
    "print(\"VGG16 Enhanced Fine-tuning Results:\")\n",
    "print(f\"  Final Training Accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"  Final Validation Accuracy: {val_accs[-1]:.2f}%\")\n",
    "print(\"VGG16 enhanced fine-tuning completed!\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zvfTsD0INuF2",
    "outputId": "b6ff317b-34d7-427d-d5a0-751857cc3715"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMMBrs4MFr_g"
   },
   "source": [
    "## AlexNET"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pzv5uB6EFr_g",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "81409ed4-b9c3-4fdb-e587-7a31a85d3361"
   },
   "source": [
    "print(\"ALEXNET ENHANCED FINE-TUNING WITH ADVANCED OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alexnet_model = AlexNetThermal(num_classes=5).to(device)\n",
    "print(f\"AlexNet model initialized with {sum(p.numel() for p in alexnet_model.parameters()):,} parameters\")\n",
    "\n",
    "\n",
    "alexnet_optimizer = optim.AdamW(\n",
    "    alexnet_model.parameters(),\n",
    "    lr=0.001,          # Starting learning rate\n",
    "    weight_decay=0.01, # Increased from 0.0005 for better regularization\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Improved learning rate schedule\n",
    "alexnet_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    alexnet_optimizer,\n",
    "    T_0=30,      # Changed from 10\n",
    "    T_mult=2,    # Restart period multiplier\n",
    "    eta_min=1e-5 # Changed from 1e-6\n",
    ")\n",
    "\n",
    "\n",
    "def train_alexnet_enhanced(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=30):\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100.0 * correct / total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc / 100)\n",
    "        val_accs.append(val_acc / 100)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1:3d}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'alexnet_best_enhanced.pth'))\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'alexnet_best_enhanced.pth')))\n",
    "    print(f\"\\nEnhanced AlexNet training completed! Best validation accuracy: {best_val_acc:.2f}%\\n\")\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "\n",
    "if alexnet_model_training:\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    train_losses, train_accs, val_losses, val_accs = train_alexnet_enhanced(\n",
    "        alexnet_model,\n",
    "        train_loader_enhanced,\n",
    "        val_loader_enhanced,\n",
    "        criterion,\n",
    "        alexnet_optimizer,\n",
    "        alexnet_scheduler,\n",
    "        epochs=common_epochs\n",
    "    )\n",
    "\n",
    "    # Save training data to CSV\n",
    "    save_training_data_to_csv('AlexNet Enhanced', train_losses, train_accs, val_losses, val_accs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model\n"
   ],
   "metadata": {
    "id": "WaKWqSn0OfRN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for AlexNet\n",
    "model_name = \"alexnet_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': alexnet_model.state_dict(),\n",
    "    'model_architecture': 'AlexNet',\n",
    "    'model_class': 'alexnet',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.5,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'CosineAnnealingWarmRestarts',\n",
    "    'scheduler_t0': 20,\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(alexnet_model, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"AlexNet Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzZk4WZvOhNt",
    "outputId": "a5de0c47-202a-458b-d787-9b65bf7cdba3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "id": "zjw1oPjzOhoV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"alexnet_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "alexnet_model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "alexnet_model.eval()\n",
    "print(\"Full alexnet_enhanced  model loaded successfully\")\n",
    "\n",
    "\n",
    "# Enhanced evaluation with correct class_names\n",
    "alexnet_results = evaluate_model_comprehensive(alexnet_model, test_loader_enhanced, categories, \"AlexNet-Enhanced\", device)\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"AlexNet\")\n",
    "\n",
    "# Store model probabilities for ROC plotting\n",
    "with torch.no_grad():\n",
    "    alexnet_model.eval()\n",
    "    test_probs = []\n",
    "    test_labels = []\n",
    "    for batch_x, batch_y in test_loader_enhanced:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = alexnet_model(batch_x)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_labels_array = np.hstack(test_labels)\n",
    "\n",
    "# Store for ROC plotting\n",
    "store_model_probabilities('AlexNet', test_probs, test_labels_array)\n",
    "MODEL_RESULTS_GLOBAL['AlexNet'] = alexnet_results\n",
    "save_training_data_to_csv(\"AlexNet\", train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "\n",
    "\n",
    "model_results.append(alexnet_results)\n",
    "\n",
    "print(\"AlexNet Enhanced Fine-tuning Results:\")\n",
    "print(f\"  Final Training Accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"  Final Validation Accuracy: {val_accs[-1]:.2f}%\")\n",
    "print(\"AlexNet enhanced fine-tuning completed!\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WF8Zbr7LOjK1",
    "outputId": "52d9a73a-689a-47a2-f88b-6910974843a1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msOaEAkgFr_g"
   },
   "source": [
    "## Hybrid Model Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "last_updated": "2025-08-11T16:45:00Z",
    "tags": [
     "enhanced-hybrid",
     "fixed-training"
    ],
    "id": "GNhLjOlnFr_g",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "a54e40ef-6e1d-4fc1-d081-175a56b4c693"
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# Fixed Enhanced Training Function\n",
    "def train_enhanced_hybrid_fixed(model, train_loader, val_loader, epochs=30):\n",
    "    \"\"\"Enhanced training with proper phase handling and metrics tracking\"\"\"\n",
    "\n",
    "    print(f\"Starting Enhanced Training for {epochs} epochs\")\n",
    "\n",
    "    # Validate data loaders\n",
    "    if len(train_loader) == 0 or len(val_loader) == 0:\n",
    "        raise ValueError(\"Data loaders cannot be empty!\")\n",
    "\n",
    "    print(f\"Data loaders validated: train={len(train_loader)}, val={len(val_loader)}\")\n",
    "\n",
    "    # Training setup\n",
    "    criterion = CombinedLoss(alpha=0.6, beta=0.4)\n",
    "\n",
    "    # Phase 1: Freeze backbone (30% of epochs)\n",
    "    freeze_epochs = max(3, int(0.3 * epochs))\n",
    "    finetune_epochs = epochs - freeze_epochs\n",
    "\n",
    "    print(f\" Phase 1: Freezing backbone for {freeze_epochs} epochs\")\n",
    "    print(f\" Phase 2: Fine-tuning all layers for {finetune_epochs} epochs\")\n",
    "\n",
    "    # Initialize tracking\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # Phase 1: Frozen backbone training\n",
    "    for param in model.vgg_features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.alex_features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                           lr=2e-3, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=freeze_epochs)\n",
    "\n",
    "    for epoch in range(freeze_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'enhanced_hybrid_best.pth')\n",
    "\n",
    "        # Progress report\n",
    "        print(f'Phase1 Epoch [{epoch+1}/{freeze_epochs}] - '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # Phase 2: Full model fine-tuning\n",
    "    print(f\" Phase 2: Fine-tuning all layers\")\n",
    "\n",
    "    # Unfreeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # New optimizer with lower learning rate for fine-tuning\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=finetune_epochs)\n",
    "\n",
    "    for epoch in range(finetune_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'enhanced_hybrid_best.pth')\n",
    "\n",
    "        # Progress report\n",
    "        print(f'Phase2 Epoch [{epoch+1}/{finetune_epochs}] - '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('enhanced_hybrid_best.pth'))\n",
    "    print(f'Enhanced Training completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
    "\n",
    "    return model, train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "# Create and train enhanced hybrid model\n",
    "print(\"CREATING ENHANCED HYBRID VGG-ALEXNET MODEL\")\n",
    "enhanced_hybrid_model = EnhancedHybridVGGAlexNet(num_classes=5, dropout_rate=0.5).to(device)\n",
    "print(f\"Enhanced Hybrid VGG-AlexNet model successfully created and moved to {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in enhanced_hybrid_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in enhanced_hybrid_model.parameters() if p.requires_grad)\n",
    "print(f\"Enhanced Model Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "\n",
    "if hybrid_model_training:\n",
    "  print(\" Starting Enhanced Hybrid VGG-AlexNet Training...\")\n",
    "  enhanced_hybrid_model, train_losses, train_accs, val_losses, val_accs = train_enhanced_hybrid_fixed(\n",
    "      enhanced_hybrid_model, train_loader_enhanced, val_loader_enhanced,  epochs= common_epochs\n",
    "  )\n",
    "\n",
    "  # Plot training curves (following ResNet pattern)\n",
    "  plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"Enhanced-Hybrid-VGG-AlexNet\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "id": "OnWQ-j4_RFQF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for Enhanced-Hybrid-VGG-AlexNet\n",
    "model_name = \"hybridmodel_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': enhanced_hybrid_model.state_dict(),\n",
    "    'model_architecture': 'Enhanced-Hybrid-VGG-AlexNet',\n",
    "    'model_class': 'EnhancedHybridVGGAlexNet',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.5,\n",
    "    'use_cbam': True,\n",
    "    'multi_scale_fusion': True,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.0005,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'CosineAnnealingLR',\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(enhanced_hybrid_model, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"Enhanced-Hybrid-VGG-AlexNet Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtVK9TLoRG89",
    "outputId": "a9042f41-e7e2-4440-dc7d-a86a72059a7a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation model"
   ],
   "metadata": {
    "id": "sEePGL0aRHN9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"hybridmodel_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "enhanced_hybrid_model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "enhanced_hybrid_model.eval()\n",
    "print(\"Full hybridmodel_enhanced  model loaded successfully\")\n",
    "\n",
    "\n",
    "print(\"Evaluating Enhanced Hybrid VGG-AlexNet Model...\")\n",
    "enhanced_hybrid_results = evaluate_model_comprehensive(enhanced_hybrid_model, test_loader_enhanced, categories, \"Enhanced-Hybrid-VGG-AlexNet\", device)\n",
    "\n",
    "# Get model probabilities and store them (following ResNet pattern)\n",
    "# Get model probabilities using direct implementation (following ResNet pattern)\n",
    "with torch.no_grad():\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    for data, target in test_loader_enhanced:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = enhanced_hybrid_model(data)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        test_labels.append(target.cpu().numpy())\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_labels_array = np.hstack(test_labels)\n",
    "store_model_probabilities('Enhanced-Hybrid-VGG-AlexNet', test_probs, test_labels_array)\n",
    "\n",
    "# Store results globally (following ResNet pattern)\n",
    "MODEL_RESULTS_GLOBAL['Enhanced-Hybrid-VGG-AlexNet'] = enhanced_hybrid_results\n",
    "save_training_data_to_csv(\"Enhanced-Hybrid-VGG-AlexNet\", train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "# Additional enhanced visualization\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Training curves\n",
    "plt.subplot(1, 3, 1)\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs_range, train_losses, 'b-', label='Training Loss', alpha=0.8)\n",
    "plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss', alpha=0.8)\n",
    "plt.axvline(x=len(train_losses)*0.3, color='gray', linestyle='--', alpha=0.7, label='Phase 1→2')\n",
    "plt.title('Enhanced Hybrid Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs_range, train_accs, 'b-', label='Training Accuracy', alpha=0.8)\n",
    "plt.plot(epochs_range, val_accs, 'r-', label='Validation Accuracy', alpha=0.8)\n",
    "plt.axvline(x=len(train_accs)*0.3, color='gray', linestyle='--', alpha=0.7, label='Phase 1→2')\n",
    "plt.title('Enhanced Hybrid Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "\n",
    "sns.heatmap(enhanced_hybrid_results['confusion_matrix'],\n",
    "           annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=categories, yticklabels=categories)\n",
    "plt.title('Enhanced Hybrid Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Enhanced Hybrid VGG-AlexNet Training Results:\")\n",
    "for key, value in enhanced_hybrid_results.items():\n",
    "    if key not in ['confusion_matrix', 'classification_report']:\n",
    "        print(f\"   {key}: {value:}\")\n",
    "\n",
    "print(\"Enhanced Hybrid VGG-AlexNet model training and evaluation completed!\")\n",
    "print(f\"Model weights saved as: enhanced_hybrid_best.pth\")\n",
    "print(f\"Final Test Accuracy: {enhanced_hybrid_results['accuracy']:.2f}%\")\n",
    "\n",
    "# Performance comparison summary\n",
    "if 'Enhanced-Hybrid-VGG-AlexNet' in MODEL_RESULTS_GLOBAL:\n",
    "    original_acc = MODEL_RESULTS_GLOBAL['Enhanced-Hybrid-VGG-AlexNet']['accuracy']\n",
    "    enhanced_acc = enhanced_hybrid_results['accuracy']\n",
    "    improvement = enhanced_acc - original_acc\n",
    "    print(f\"PERFORMANCE IMPROVEMENT:\")\n",
    "    print(f\"   Original Hybrid: {original_acc:.2f}%\")\n",
    "    print(f\"   Enhanced Hybrid: {enhanced_acc:.2f}%\")\n",
    "    print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(f\"Enhanced Hybrid Model Performance:\")\n",
    "    print(f\"   Test Accuracy: {enhanced_hybrid_results['accuracy']:.2f}%\")\n",
    "    print(f\"   F1-Score: {enhanced_hybrid_results.get('f1_macro', 0):.2f}%\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9EB-RVu5RIkt",
    "outputId": "35c778e7-49af-46ee-f5a4-9df59c9fe53d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m13X515fFr_g"
   },
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EXND185-Fr_g",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "00cce847-5aa7-4f5a-ff90-97041a0f4518"
   },
   "source": [
    "# Enhanced CNN Architecture Implementation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ENHANCED CNN ARCHITECTURE TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"CNN model successfully moved to {device}\")\n",
    "\n",
    "# Create CNN model\n",
    "num_classes = len(categories)\n",
    "cnn_enhanced = ProposedCNN(input_shape=(3, 200, 200), num_classes=num_classes, dropout_rate=0.3)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in cnn_enhanced.parameters())\n",
    "trainable_params = sum(p.numel() for p in cnn_enhanced.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n CNN Architecture Summary:\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train with validation\n",
    "#epochs = 5\n",
    "if cnn_model_training:\n",
    "  epochs = common_epochs\n",
    "  train_losses, train_accs, val_losses, val_accs = train_model_fine_tuned(\n",
    "      cnn_enhanced, train_loader_enhanced, val_loader_enhanced,\n",
    "      criterion, epochs, device, \"ProposedCNN\")\n",
    "\n",
    "  # Plot training curves\n",
    "  plot_training_curves(train_losses, train_accs, val_losses, val_accs, \"ProposedCNN\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model"
   ],
   "metadata": {
    "id": "ZHvNGvbzRf-d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Comprehensive model saving for ProposedCNN\n",
    "model_name = \"cnn_model_epoch_100\"\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create comprehensive checkpoint\n",
    "checkpoint = {\n",
    "    # Model architecture and weights\n",
    "    'model_state_dict': cnn_enhanced.state_dict(),\n",
    "    'model_architecture': 'ProposedCNN',\n",
    "    'model_class': 'ProposedCNN',\n",
    "    \n",
    "    # Training configuration\n",
    "    'num_classes': len(categories),\n",
    "    'class_names': categories,\n",
    "    'epochs_trained': common_epochs,\n",
    "    'dropout_rate': 0.5,\n",
    "    'use_inception_modules': True,\n",
    "    'multi_branch_architecture': True,\n",
    "    \n",
    "    # Training history\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'best_val_acc': max(val_accs),\n",
    "    'best_val_loss': min(val_losses),\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    'optimizer_name': 'AdamW',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_name': 'CosineAnnealingLR',\n",
    "    \n",
    "    # Metadata\n",
    "    'timestamp': timestamp,\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_size': (200, 200),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Save full model (for easy loading)\n",
    "full_model_path = os.path.join(MODEL_DIR, f\"{model_name}_full.pth\")\n",
    "torch.save(cnn_enhanced, full_model_path)\n",
    "\n",
    "# Save comprehensive checkpoint (recommended for production)\n",
    "checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_checkpoint.pth\")\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"ProposedCNN Model saved successfully:\")\n",
    "print(f\"   Full model: {full_model_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Best validation accuracy: {max(val_accs):.4f}\")\n",
    "print(f\"   Best validation loss: {min(val_losses):.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI99yzfKRhh1",
    "outputId": "6b0f153b-f6cb-4056-f6d6-64b2eb19762b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "id": "Qmj35tQZRhtM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"cnn_model_epoch_100_full\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "cnnmodel_enhanced = torch.load(model_path, map_location=device, weights_only=False)\n",
    "cnnmodel_enhanced.eval()\n",
    "print(\"Full hybridmodel_enhanced  model loaded successfully\")\n",
    "\n",
    "# Comprehensive evaluation\n",
    "cnn_results = evaluate_model_comprehensive(cnn_enhanced, test_loader_enhanced, categories, \"ProposedCNN\", device)\n",
    "\n",
    "# Store model probabilities for ROC plotting\n",
    "with torch.no_grad():\n",
    "    cnn_enhanced.eval()\n",
    "    test_probs = []\n",
    "    test_labels = []\n",
    "    for batch_x, batch_y in test_loader_enhanced:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = cnn_enhanced(batch_x)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_labels_array = np.hstack(test_labels)\n",
    "\n",
    "# Store for ROC plotting\n",
    "store_model_probabilities('ProposedCNN', test_probs, test_labels_array)\n",
    "MODEL_RESULTS_GLOBAL['ProposedCNN'] = cnn_results\n",
    "save_training_data_to_csv(\"ProposedCNN\", train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "\n",
    "\n",
    "# Store results\n",
    "model_results.append(cnn_results)\n",
    "print(f\"\\nProposed CNN training and evaluation completed!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aMMRbFIWRj_s",
    "outputId": "d5b0f6a3-1b21-4410-e4ed-597e88ea4103"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwGPEF9iFr_g"
   },
   "source": [
    "# Data Analysis Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "id": "zSXCkan1Fr_h"
   },
   "source": [
    "## Comprehensive Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VWuwPnrKFr_h"
   },
   "source": [
    "# Comprehensive Model Comparison and Final Results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" COMPREHENSIVE MODEL COMPARISON - FINAL RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Create comparison table\n",
    "\n",
    "if model_results:\n",
    "    # Create DataFrame for easy comparison\n",
    "    comparison_df = pd.DataFrame(model_results)\n",
    "\n",
    "    # Display results table\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<12} {'Recall':<10} {'F1-Score':<10} {'F1-Weighted':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        print(f\"{row['model_name']:<20} {row['accuracy']:<10.4f} {row['precision_macro']:<12.4f} \"\n",
    "              f\"{row['recall_macro']:<10.4f} {row['f1_macro']:<10.4f} {row['f1_weighted']:<12.4f}\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Find best performing model for each metric\n",
    "    best_accuracy = comparison_df.loc[comparison_df['accuracy'].idxmax()]\n",
    "    best_precision = comparison_df.loc[comparison_df['precision_macro'].idxmax()]\n",
    "    best_recall = comparison_df.loc[comparison_df['recall_macro'].idxmax()]\n",
    "    best_f1 = comparison_df.loc[comparison_df['f1_macro'].idxmax()]\n",
    "\n",
    "    print(f\"\\n Best Performing Models:\")\n",
    "    print(f\"  Highest Accuracy: {best_accuracy['model_name']} ({best_accuracy['accuracy']:.4f})\")\n",
    "    print(f\"  Highest Precision: {best_precision['model_name']} ({best_precision['precision_macro']:.4f})\")\n",
    "    print(f\"  Highest Recall: {best_recall['model_name']} ({best_recall['recall_macro']:.4f})\")\n",
    "    print(f\"  Highest F1-Score: {best_f1['model_name']} ({best_f1['f1_macro']:.4f})\")\n",
    "\n",
    "    # Create comparison plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    models = comparison_df['model_name']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple'][:len(models)]\n",
    "\n",
    "    # Accuracy comparison\n",
    "    accuracies = comparison_df['accuracy']\n",
    "    bars1 = ax1.bar(models, accuracies, color=colors)\n",
    "    ax1.set_title('Model Accuracy Comparison', fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Precision comparison\n",
    "    precisions = comparison_df['precision_macro']\n",
    "    bars2 = ax2.bar(models, precisions, color=colors)\n",
    "    ax2.set_title('Model Precision Comparison', fontweight='bold')\n",
    "    ax2.set_ylabel('Precision (Macro)')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    for bar, prec in zip(bars2, precisions):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{prec:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Recall comparison\n",
    "    recalls = comparison_df['recall_macro']\n",
    "    bars3 = ax3.bar(models, recalls, color=colors)\n",
    "    ax3.set_title('Model Recall Comparison', fontweight='bold')\n",
    "    ax3.set_ylabel('Recall (Macro)')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    for bar, rec in zip(bars3, recalls):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{rec:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # F1-Score comparison\n",
    "    f1_scores = comparison_df['f1_macro']\n",
    "    bars4 = ax4.bar(models, f1_scores, color=colors)\n",
    "    ax4.set_title('Model F1-Score Comparison', fontweight='bold')\n",
    "    ax4.set_ylabel('F1-Score (Macro)')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    for bar, f1 in zip(bars4, f1_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(' Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Determine overall best model\n",
    "    comparison_df['overall_score'] = (comparison_df['accuracy'] + comparison_df['precision_macro'] +\n",
    "                                     comparison_df['recall_macro'] + comparison_df['f1_macro']) / 4\n",
    "    best_overall = comparison_df.loc[comparison_df['overall_score'].idxmax()]\n",
    "\n",
    "    print(f\"\\n OVERALL BEST MODEL: {best_overall['model_name']}\")\n",
    "    print(f\"   Overall Score: {best_overall['overall_score']:.4f}\")\n",
    "    print(f\"   Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {best_overall['precision_macro']:.4f}\")\n",
    "    print(f\"   Recall: {best_overall['recall_macro']:.4f}\")\n",
    "    print(f\"   F1-Score: {best_overall['f1_macro']:.4f}\")\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nFinal Summary:\")\n",
    "    print(f\"  Dataset: {len(X_train) + len(X_val) + len(X_test)} total samples (augmented)\")\n",
    "    print(f\"   Models trained: {len(model_results)}\")\n",
    "    print(f\"  Optimizations used: 8 advanced techniques\")\n",
    "    print(f\"   Hardware: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(f\"  Best overall accuracy: {comparison_df['accuracy'].max():.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No model results found. Please run the model training sections first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ENHANCED THERMAL IMAGING CLASSIFICATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\" All models trained with comprehensive optimization suite\")\n",
    "print(\"Detailed evaluation metrics calculated and compared\")\n",
    "print(\" Best performing models identified\")\n",
    "print(\"=\"*100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTEfY1sfFr_h"
   },
   "source": [
    "## CSV Exporter"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "edjZJfmsFr_h"
   },
   "source": [
    "# Enhanced Data Export and CSV Generation for Graph Creation\n",
    "# =========================================================\n",
    "# This cell provides comprehensive CSV export functionality for all thermal imaging analysis data\n",
    "# Add this cell to your notebook after the \"Enhanced Data Export and Comprehensive Visualization Execution\" section\n",
    "\n",
    "class ThermalDataCSVExporter:\n",
    "    \"\"\"Comprehensive CSV exporter for thermal imaging analysis data\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir='thermal_csv_exports'):\n",
    "        \"\"\"Initialize the CSV exporter with output directory\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        print(f\"CSV export directory: {self.output_dir.absolute()}\")\n",
    "\n",
    "    def export_training_data_csv(self, training_data_dict):\n",
    "        \"\"\"Export training curves data to CSV files for graph creation\"\"\"\n",
    "        print(\"Exporting training data to CSV...\")\n",
    "\n",
    "        # Combined training data for all models\n",
    "        combined_data = []\n",
    "\n",
    "        for model_name, data in training_data_dict.items():\n",
    "            epochs = range(1, len(data['train_losses']) + 1)\n",
    "\n",
    "            # Create individual model DataFrame\n",
    "            df_individual = pd.DataFrame({\n",
    "                'epoch': epochs,\n",
    "                'train_loss': data['train_losses'],\n",
    "                'train_accuracy': data['train_accs'],\n",
    "                'val_loss': data['val_losses'],\n",
    "                'val_accuracy': data['val_accs'],\n",
    "                'model': model_name,\n",
    "                'loss_difference': np.array(data['val_losses']) - np.array(data['train_losses']),\n",
    "                'accuracy_difference': np.array(data['train_accs']) - np.array(data['val_accs'])\n",
    "            })\n",
    "\n",
    "            # Save individual model file\n",
    "            filename = f'{model_name.lower().replace(\" \", \"_\")}_training_curves_{self.timestamp}.csv'\n",
    "            filepath = self.output_dir / filename\n",
    "            df_individual.to_csv(filepath, index=False)\n",
    "            print(f\"  Saved: {filename}\")\n",
    "\n",
    "            # Add to combined data\n",
    "            combined_data.append(df_individual)\n",
    "\n",
    "        # Save combined training data\n",
    "        if combined_data:\n",
    "            df_combined = pd.concat(combined_data, ignore_index=True)\n",
    "            combined_filepath = self.output_dir / f'all_models_training_curves_{self.timestamp}.csv'\n",
    "            df_combined.to_csv(combined_filepath, index=False)\n",
    "            print(f\"  Saved combined training data: all_models_training_curves_{self.timestamp}.csv\")\n",
    "\n",
    "        return len(training_data_dict)\n",
    "\n",
    "    def export_model_comparison_csv(self, model_results_list):\n",
    "        \"\"\"Export model comparison metrics to CSV for comparison graphs\"\"\"\n",
    "        print(\"Exporting model comparison data to CSV...\")\n",
    "\n",
    "        if not model_results_list:\n",
    "            print(\"  No model results to export\")\n",
    "            return 0\n",
    "\n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for result in model_results_list:\n",
    "            comparison_data.append({\n",
    "                'model_name': result.get('model_name', 'Unknown'),\n",
    "                'accuracy': result.get('accuracy', 0) * 100,  # Convert to percentage\n",
    "                'precision_macro': result.get('precision_macro', 0) * 100,\n",
    "                'recall_macro': result.get('recall_macro', 0) * 100,\n",
    "                'f1_macro': result.get('f1_macro', 0) * 100,\n",
    "                'precision_micro': result.get('precision_micro', 0) * 100,\n",
    "                'recall_micro': result.get('recall_micro', 0) * 100,\n",
    "                'f1_micro': result.get('f1_micro', 0) * 100,\n",
    "                'precision_weighted': result.get('precision_weighted', 0) * 100,\n",
    "                'recall_weighted': result.get('recall_weighted', 0) * 100,\n",
    "                'f1_weighted': result.get('f1_weighted', 0) * 100\n",
    "            })\n",
    "\n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        filepath = self.output_dir / f'model_comparison_{self.timestamp}.csv'\n",
    "        df_comparison.to_csv(filepath, index=False)\n",
    "        print(f\"  Saved: model_comparison_{self.timestamp}.csv\")\n",
    "\n",
    "        return len(model_results_list)\n",
    "\n",
    "# Execute the CSV export using existing data from the notebook\n",
    "# =============================================================\n",
    "\n",
    "# Initialize the CSV exporter\n",
    "csv_exporter = ThermalDataCSVExporter()\n",
    "\n",
    "print(\"Starting comprehensive CSV export for graph creation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export training data if available\n",
    "if 'enhanced_training_data' in globals() and enhanced_training_data:\n",
    "    print(\"Exporting training curves data...\")\n",
    "    csv_exporter.export_training_data_csv(enhanced_training_data)\n",
    "    print()\n",
    "\n",
    "# Export model comparison data if available\n",
    "if 'enhanced_model_results' in globals() and enhanced_model_results:\n",
    "    print(\"Exporting model comparison data...\")\n",
    "    csv_exporter.export_model_comparison_csv(enhanced_model_results)\n",
    "    print()\n",
    "\n",
    "# Also export any other model results variables that might exist\n",
    "alternative_vars = ['model_results', 'all_model_results', 'final_results']\n",
    "for var_name in alternative_vars:\n",
    "    if var_name in globals() and globals()[var_name]:\n",
    "        print(f\"Found additional results in '{var_name}', exporting...\")\n",
    "        csv_exporter.export_model_comparison_csv(globals()[var_name])\n",
    "        print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CSV EXPORT COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"All CSV files saved to: {csv_exporter.output_dir.absolute()}\")\n",
    "print()\n",
    "print(\"Available CSV files for creating graphs:\")\n",
    "print(\"  • all_models_training_curves_*.csv - For training/validation loss and accuracy plots\")\n",
    "print(\"  • model_comparison_*.csv - For model performance comparison charts\")\n",
    "print(\"  • Individual model files - For detailed per-model analysis\")\n",
    "print()\n",
    "print(\" You can now use these CSV files with any plotting library (matplotlib, seaborn, plotly)\")\n",
    "print(\"   or external tools (Excel, Tableau, Power BI) to create custom visualizations!\")\n",
    "\n",
    "# Display sample of the exported data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE OF EXPORTED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show sample of training data if available\n",
    "training_files = list(csv_exporter.output_dir.glob(\"all_models_training_curves_*.csv\"))\n",
    "if training_files:\n",
    "    sample_df = pd.read_csv(training_files[0])\n",
    "    print(\"Training Curves Data Sample:\")\n",
    "    print(sample_df.head())\n",
    "    print(f\"Shape: {sample_df.shape}\")\n",
    "    print()\n",
    "\n",
    "# Show sample of comparison data if available\n",
    "comparison_files = list(csv_exporter.output_dir.glob(\"model_comparison_*.csv\"))\n",
    "if comparison_files:\n",
    "    sample_df = pd.read_csv(comparison_files[0])\n",
    "    print(\"Model Comparison Data Sample:\")\n",
    "    print(sample_df)\n",
    "    print(f\"Shape: {sample_df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "id": "MoAT_DInFr_h"
   },
   "source": [
    "## Data Visualiser"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XD4XjRdWFr_h"
   },
   "source": [
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class ThermalDataVisualizer:\n",
    "    \"\"\"Create comprehensive visualizations from thermal imaging CSV data\"\"\"\n",
    "\n",
    "    def __init__(self, csv_directory='thermal_csv_exports'):\n",
    "        \"\"\"Initialize with the directory containing CSV files\"\"\"\n",
    "        self.csv_dir = Path(csv_directory)\n",
    "        if not self.csv_dir.exists():\n",
    "            raise FileNotFoundError(f\"CSV directory not found: {csv_directory}\")\n",
    "\n",
    "        self.output_dir = self.csv_dir / 'visualizations'\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        print(f\"CSV Data Directory: {self.csv_dir.absolute()}\")\n",
    "        print(f\"Visualization Output: {self.output_dir.absolute()}\")\n",
    "\n",
    "        # Load available CSV files\n",
    "        self.load_csv_files()\n",
    "\n",
    "    def load_csv_files(self):\n",
    "        \"\"\"Load all available CSV files\"\"\"\n",
    "        print(\"\\n Loading CSV files...\")\n",
    "\n",
    "        # Find training curves data\n",
    "        training_files = list(self.csv_dir.glob(\"all_models_training_curves_*.csv\"))\n",
    "        self.training_data = None\n",
    "        if training_files:\n",
    "            self.training_data = pd.read_csv(training_files[0])\n",
    "            print(f\"  Training data loaded: {training_files[0].name}\")\n",
    "\n",
    "        # Find model comparison data\n",
    "        comparison_files = list(self.csv_dir.glob(\"model_comparison_*.csv\"))\n",
    "        self.comparison_data = None\n",
    "        if comparison_files:\n",
    "            self.comparison_data = pd.read_csv(comparison_files[0])\n",
    "            print(f\"  Comparison data loaded: {comparison_files[0].name}\")\n",
    "\n",
    "        # Find individual model files\n",
    "        individual_files = list(self.csv_dir.glob(\"*_training_curves_*.csv\"))\n",
    "        individual_files = [f for f in individual_files if \"all_models\" not in f.name]\n",
    "        self.individual_data = {}\n",
    "        for file in individual_files:\n",
    "            model_name = file.name.split('_training_curves_')[0]\n",
    "            self.individual_data[model_name] = pd.read_csv(file)\n",
    "            print(f\"  Individual model data loaded: {model_name}\")\n",
    "\n",
    "    def plot_training_curves_comparison(self, save=True, figsize=(15, 12)):\n",
    "        \"\"\"Create training curves comparison plots\"\"\"\n",
    "        if self.training_data is None:\n",
    "            print(\"No training data available for plotting\")\n",
    "            return\n",
    "\n",
    "        print(\"Creating training curves comparison plots...\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        fig.suptitle('Training Curves Comparison Across Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "        models = self.training_data['model'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(models)))\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            model_data = self.training_data[self.training_data['model'] == model]\n",
    "            color = colors[i]\n",
    "\n",
    "            # Training Loss\n",
    "            axes[0, 0].plot(model_data['epoch'], model_data['train_loss'],\n",
    "                           label=f'{model}', color=color, marker='o', linewidth=2)\n",
    "\n",
    "            # Validation Loss\n",
    "            axes[0, 1].plot(model_data['epoch'], model_data['val_loss'],\n",
    "                           label=f'{model}', color=color, marker='s', linewidth=2)\n",
    "\n",
    "            # Training Accuracy\n",
    "            axes[1, 0].plot(model_data['epoch'], model_data['train_accuracy'],\n",
    "                           label=f'{model}', color=color, marker='^', linewidth=2)\n",
    "\n",
    "            # Validation Accuracy\n",
    "            axes[1, 1].plot(model_data['epoch'], model_data['val_accuracy'],\n",
    "                           label=f'{model}', color=color, marker='D', linewidth=2)\n",
    "\n",
    "        # Customize subplots\n",
    "        axes[0, 0].set_title('Training Loss', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[0, 1].set_title('Validation Loss', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1, 0].set_title('Training Accuracy', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Accuracy')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1, 1].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / 'training_curves_comparison.png'\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"  Saved: {filepath.name}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_overfitting_analysis(self, save=True, figsize=(15, 6)):\n",
    "        \"\"\"Create overfitting analysis plots\"\"\"\n",
    "        if self.training_data is None:\n",
    "            print(\"No training data available for overfitting analysis\")\n",
    "            return\n",
    "\n",
    "        print(\" Creating overfitting analysis plots...\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        fig.suptitle('Overfitting Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "        models = self.training_data['model'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(models)))\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            model_data = self.training_data[self.training_data['model'] == model]\n",
    "            color = colors[i]\n",
    "\n",
    "            # Loss difference (Val - Train)\n",
    "            axes[0].plot(model_data['epoch'], model_data['loss_difference'],\n",
    "                        label=f'{model}', color=color, marker='o', linewidth=2)\n",
    "\n",
    "            # Accuracy difference (Train - Val)\n",
    "            axes[1].plot(model_data['epoch'], model_data['accuracy_difference'],\n",
    "                        label=f'{model}', color=color, marker='s', linewidth=2)\n",
    "\n",
    "        # Customize subplots\n",
    "        axes[0].set_title('Loss Difference (Validation - Training)', fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss Difference')\n",
    "        axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].set_title('Accuracy Difference (Training - Validation)', fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy Difference')\n",
    "        axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / 'overfitting_analysis.png'\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"  Saved: {filepath.name}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_model_comparison(self, save=True, figsize=(15, 12)):\n",
    "        \"\"\"Create model performance comparison plots\"\"\"\n",
    "        if self.comparison_data is None:\n",
    "            print(\"No comparison data available for plotting\")\n",
    "            return\n",
    "\n",
    "        print(\"Creating model comparison plots...\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Main metrics for bar plot\n",
    "        main_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "        # Bar plot for main metrics\n",
    "        x = np.arange(len(self.comparison_data))\n",
    "        width = 0.2\n",
    "\n",
    "        for i, metric in enumerate(main_metrics):\n",
    "            if metric in self.comparison_data.columns:\n",
    "                axes[0, 0].bar(x + i*width, self.comparison_data[metric],\n",
    "                              width, label=metric.replace('_', ' ').title())\n",
    "\n",
    "        axes[0, 0].set_title('Main Performance Metrics', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Models')\n",
    "        axes[0, 0].set_ylabel('Score (%)')\n",
    "        axes[0, 0].set_xticks(x + width * 1.5)\n",
    "        axes[0, 0].set_xticklabels(self.comparison_data['model_name'], rotation=45)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Heatmap for all metrics\n",
    "        metric_cols = [col for col in self.comparison_data.columns if col != 'model_name']\n",
    "        heatmap_data = self.comparison_data[metric_cols].T\n",
    "        heatmap_data.columns = self.comparison_data['model_name']\n",
    "\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "                   ax=axes[0, 1], cbar_kws={'label': 'Score (%)'})\n",
    "        axes[0, 1].set_title('Performance Heatmap', fontweight='bold')\n",
    "\n",
    "        # Radar chart for main metrics\n",
    "        if len(main_metrics) >= 3:\n",
    "            angles = np.linspace(0, 2*np.pi, len(main_metrics), endpoint=False).tolist()\n",
    "            angles += angles[:1]  # Complete the circle\n",
    "\n",
    "            ax_radar = plt.subplot(2, 2, 3, projection='polar')\n",
    "\n",
    "            for i, model in enumerate(self.comparison_data['model_name']):\n",
    "                values = []\n",
    "                for metric in main_metrics:\n",
    "                    if metric in self.comparison_data.columns:\n",
    "                        values.append(self.comparison_data.iloc[i][metric])\n",
    "                values += values[:1]  # Complete the circle\n",
    "\n",
    "                ax_radar.plot(angles, values, 'o-', linewidth=2, label=model)\n",
    "                ax_radar.fill(angles, values, alpha=0.25)\n",
    "\n",
    "            ax_radar.set_xticks(angles[:-1])\n",
    "            ax_radar.set_xticklabels([m.replace('_', ' ').title() for m in main_metrics])\n",
    "            ax_radar.set_title('Performance Radar Chart', fontweight='bold', pad=20)\n",
    "            ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "        # Accuracy comparison bar plot\n",
    "        if 'accuracy' in self.comparison_data.columns:\n",
    "            bars = axes[1, 1].bar(self.comparison_data['model_name'],\n",
    "                                 self.comparison_data['accuracy'],\n",
    "                                 color=plt.cm.viridis(np.linspace(0, 1, len(self.comparison_data))))\n",
    "\n",
    "            axes[1, 1].set_title('Accuracy Comparison', fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Models')\n",
    "            axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "            axes[1, 1].set_xticklabels(self.comparison_data['model_name'], rotation=45)\n",
    "\n",
    "            # Add value labels on bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                axes[1, 1].annotate(f'{height:.2f}%',\n",
    "                                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                                   xytext=(0, 3),  # 3 points vertical offset\n",
    "                                   textcoords=\"offset points\",\n",
    "                                   ha='center', va='bottom')\n",
    "\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / 'model_comparison.png'\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"  Saved: {filepath.name}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def create_summary_report(self, save=True, figsize=(12, 10)):\n",
    "        \"\"\"Create a summary report with key metrics\"\"\"\n",
    "        print(\"Creating summary report...\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
    "        fig.suptitle('Thermal Imaging Analysis Summary Report', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Training summary\n",
    "        if self.training_data is not None:\n",
    "            summary_text = \"TRAINING SUMMARY\\n\" + \"=\"*50 + \"\\n\"\n",
    "\n",
    "            for model in self.training_data['model'].unique():\n",
    "                model_data = self.training_data[self.training_data['model'] == model]\n",
    "                final_train_acc = model_data['train_accuracy'].iloc[-1]\n",
    "                final_val_acc = model_data['val_accuracy'].iloc[-1]\n",
    "                final_train_loss = model_data['train_loss'].iloc[-1]\n",
    "                final_val_loss = model_data['val_loss'].iloc[-1]\n",
    "\n",
    "                summary_text += f\"\\n{model}:\\n\"\n",
    "                summary_text += f\"  Final Training Accuracy: {final_train_acc:.4f}\\n\"\n",
    "                summary_text += f\"  Final Validation Accuracy: {final_val_acc:.4f}\\n\"\n",
    "                summary_text += f\"  Final Training Loss: {final_train_loss:.4f}\\n\"\n",
    "                summary_text += f\"  Final Validation Loss: {final_val_loss:.4f}\\n\"\n",
    "\n",
    "            axes[0].text(0.05, 0.95, summary_text, transform=axes[0].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "            axes[0].set_title('Training Results Summary', fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "\n",
    "        # Model comparison summary\n",
    "        if self.comparison_data is not None:\n",
    "            comparison_text = \"MODEL COMPARISON\\n\" + \"=\"*50 + \"\\n\"\n",
    "\n",
    "            best_accuracy = self.comparison_data.loc[self.comparison_data['accuracy'].idxmax()]\n",
    "            comparison_text += f\"\\nBest Overall Model: {best_accuracy['model_name']}\\n\"\n",
    "            comparison_text += f\"  Accuracy: {best_accuracy['accuracy']:.2f}%\\n\"\n",
    "\n",
    "            if 'precision_macro' in self.comparison_data.columns:\n",
    "                comparison_text += f\"  Precision (Macro): {best_accuracy['precision_macro']:.2f}%\\n\"\n",
    "            if 'recall_macro' in self.comparison_data.columns:\n",
    "                comparison_text += f\"  Recall (Macro): {best_accuracy['recall_macro']:.2f}%\\n\"\n",
    "            if 'f1_macro' in self.comparison_data.columns:\n",
    "                comparison_text += f\"  F1-Score (Macro): {best_accuracy['f1_macro']:.2f}%\\n\"\n",
    "\n",
    "            comparison_text += f\"\\nAll Models Performance:\\n\"\n",
    "            for _, row in self.comparison_data.iterrows():\n",
    "                comparison_text += f\"  {row['model_name']}: {row['accuracy']:.2f}% accuracy\\n\"\n",
    "\n",
    "            axes[1].text(0.05, 0.95, comparison_text, transform=axes[1].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "            axes[1].set_title('Performance Comparison Summary', fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / 'summary_report.png'\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"  Saved: {filepath.name}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def generate_all_plots(self):\n",
    "        \"\"\"Generate all available plots\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"GENERATING ALL VISUALIZATION PLOTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Generate all plots\n",
    "        if self.training_data is not None:\n",
    "            self.plot_training_curves_comparison()\n",
    "            print()\n",
    "            self.plot_overfitting_analysis()\n",
    "            print()\n",
    "\n",
    "        if self.comparison_data is not None:\n",
    "            self.plot_model_comparison()\n",
    "            print()\n",
    "\n",
    "        self.create_summary_report()\n",
    "        print()\n",
    "\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"ALL VISUALIZATIONS COMPLETED!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"All plots saved to: {self.output_dir.absolute()}\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        for plot_file in self.output_dir.glob(\"*.png\"):\n",
    "            print(f\"  • {plot_file.name}\")\n",
    "\n",
    "# Convenience functions for quick usage\n",
    "def create_all_visualizations(csv_directory='thermal_csv_exports'):\n",
    "    \"\"\"One-line function to create all visualizations\"\"\"\n",
    "    try:\n",
    "        visualizer = ThermalDataVisualizer(csv_directory)\n",
    "        visualizer.generate_all_plots()\n",
    "        return visualizer\n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_data_summary(csv_directory='thermal_csv_exports'):\n",
    "    \"\"\"Get a summary of available data\"\"\"\n",
    "    try:\n",
    "        visualizer = ThermalDataVisualizer(csv_directory)\n",
    "\n",
    "        print(\"DATA SUMMARY\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"CSV Directory: {visualizer.csv_dir.absolute()}\")\n",
    "        print(f\" Training Data Available: {'' if visualizer.training_data is not None else ''}\")\n",
    "        print(f\"Comparison Data Available: {'' if visualizer.comparison_data is not None else ''}\")\n",
    "\n",
    "        if visualizer.training_data is not None:\n",
    "            models = list(visualizer.training_data['model'].unique())\n",
    "            print(f\" Models Found: {', '.join(models)}\")\n",
    "            print(f\" Training Data Shape: {visualizer.training_data.shape}\")\n",
    "\n",
    "        if visualizer.comparison_data is not None:\n",
    "            print(f\" Comparison Data Shape: {visualizer.comparison_data.shape}\")\n",
    "\n",
    "        return visualizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Visualization functions loaded successfully!\")\n",
    "print(\" Use create_all_visualizations() to generate all plots, or get_data_summary() to check available data\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "id": "X35M3oo1Fr_o"
   },
   "source": [
    "## Image Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jJp3KS2VFr_o"
   },
   "source": [
    "print(\"CREATING THERMAL IMAGING VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if CSV data exists and create visualizations\n",
    "try:\n",
    "    # Get data summary first\n",
    "    print(\" Checking available data...\")\n",
    "    visualizer = get_data_summary()\n",
    "\n",
    "    if visualizer and (visualizer.training_data is not None or visualizer.comparison_data is not None):\n",
    "        print(\"\\nCreating all visualizations...\")\n",
    "        visualizer.generate_all_plots()\n",
    "\n",
    "        print(\"\\nVISUALIZATION COMPLETE!\")\n",
    "        print(\"Check the 'thermal_csv_exports/visualizations' folder for saved plots\")\n",
    "        print(\" All plots are displayed above and saved as high-quality PNG files\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n No data available for visualization!\")\n",
    "        print(\" Make sure you've run the CSV export cell first\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error creating visualizations: {e}\")\n",
    "    print(\" Make sure the CSV export completed successfully\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HyaubYQFr_o"
   },
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VdtLhOfwFr_o"
   },
   "source": [
    "# Generate comprehensive ROC analysis for all trained models\n",
    "print(\"\\nGenerating Comprehensive ROC Analysis for All Models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "final_auc_scores = plot_all_model_roc_curves()\n",
    "\n",
    "if final_auc_scores:\n",
    "    print(\"\\nFinal AUC Summary:\")\n",
    "    model_names = list(MODEL_RESULTS_GLOBAL.keys())\n",
    "    for model_name, auc_score in zip(model_names, final_auc_scores):\n",
    "        print(f\"   {model_name}: {auc_score:.4f}\")\n",
    "\n",
    "    # Find best model\n",
    "    best_idx = np.argmax(final_auc_scores)\n",
    "    best_model = model_names[best_idx]\n",
    "    best_auc = final_auc_scores[best_idx]\n",
    "\n",
    "    print(f\"\\n Best Model: {best_model} (AUC: {best_auc:.4f})\")\n",
    "\n",
    "print(\"\\nEnhanced ROC Analysis Complete!\")\n",
    "print(\"Check the 'thermal_analysis_results' folder for generated plots:\")\n",
    "print(\"   • enhanced_roc_curves_seaborn.png\")\n",
    "print(\"   • enhanced_roc_dashboard_seaborn.png\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MLVtM6uOFr_o"
   },
   "source": [
    "# Next Steps: Comprehensive Model Analysis and Visualization\n",
    "# Add this cell after all your model training and CSV export is complete\n",
    "\n",
    "print(\"Starting Comprehensive Model Analysis...\")\n",
    "\n",
    "# Step 1: Plot individual training curves for each model\n",
    "print(\"\\nStep 1: Plotting individual training curves...\")\n",
    "\n",
    "results_dir = 'thermal_analysis_results'\n",
    "csv_files = glob.glob(os.path.join(results_dir, '*_training_curves_*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} training curve files:\")\n",
    "for csv_file in csv_files:\n",
    "    filename = os.path.basename(csv_file)\n",
    "    print(f\"   {filename}\")\n",
    "\n",
    "# Plot each model's training curves individually\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        model_name = df['model_name'].iloc[0]\n",
    "        print(f\"\\nPlotting training curves for: {model_name}\")\n",
    "        plot_training_curves_from_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {csv_file}: {e}\")\n",
    "\n",
    "# Step 2: Create comparison of all models\n",
    "print(\"\\nStep 2: Creating comparison of all models...\")\n",
    "plot_all_models_comparison()\n",
    "\n",
    "# Step 3: Generate training summary\n",
    "print(\"\\nStep 3: Generating training summary...\")\n",
    "training_summary = create_training_summary()\n",
    "\n",
    "# Step 4: Plot comprehensive model evaluation metrics\n",
    "print(\"\\nStep 4: Creating comprehensive evaluation metrics visualization...\")\n",
    "\n",
    "# Check if we have model evaluation results\n",
    "if 'model_results' in globals() and model_results:\n",
    "    # Prepare evaluation metrics data\n",
    "    evaluation_data = []\n",
    "    for result in model_results:\n",
    "        evaluation_data.append({\n",
    "            'Model': result.get('model_name', 'Unknown'),\n",
    "            'Accuracy': result.get('accuracy', 0) * 100,\n",
    "            'Precision (Macro)': result.get('precision_macro', 0) * 100,\n",
    "            'Recall (Macro)': result.get('recall_macro', 0) * 100,\n",
    "            'F1-Score (Macro)': result.get('f1_macro', 0) * 100,\n",
    "            'Precision (Weighted)': result.get('precision_weighted', 0) * 100,\n",
    "            'Recall (Weighted)': result.get('recall_weighted', 0) * 100,\n",
    "            'F1-Score (Weighted)': result.get('f1_weighted', 0) * 100\n",
    "        })\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "\n",
    "    # Create comprehensive evaluation metrics plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    models = eval_df['Model']\n",
    "    x_pos = np.arange(len(models))\n",
    "\n",
    "    # Plot 1: Accuracy vs F1-Scores\n",
    "    width = 0.25\n",
    "    axes[0,0].bar(x_pos - width, eval_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "    axes[0,0].bar(x_pos, eval_df['F1-Score (Macro)'], width, label='F1-Macro', alpha=0.8)\n",
    "    axes[0,0].bar(x_pos + width, eval_df['F1-Score (Weighted)'], width, label='F1-Weighted', alpha=0.8)\n",
    "    axes[0,0].set_title('Model Performance Comparison', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Models')\n",
    "    axes[0,0].set_ylabel('Performance (%)')\n",
    "    axes[0,0].set_xticks(x_pos)\n",
    "    axes[0,0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, model in enumerate(models):\n",
    "        axes[0,0].text(i - width, eval_df.iloc[i]['Accuracy'] + 1, f\"{eval_df.iloc[i]['Accuracy']:.1f}\",\n",
    "                      ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        axes[0,0].text(i, eval_df.iloc[i]['F1-Score (Macro)'] + 1, f\"{eval_df.iloc[i]['F1-Score (Macro)']:.1f}\",\n",
    "                      ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        axes[0,0].text(i + width, eval_df.iloc[i]['F1-Score (Weighted)'] + 1, f\"{eval_df.iloc[i]['F1-Score (Weighted)']:.1f}\",\n",
    "                      ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "    # Plot 2: Precision Comparison\n",
    "    axes[0,1].bar(x_pos - width/2, eval_df['Precision (Macro)'], width, label='Macro', alpha=0.8)\n",
    "    axes[0,1].bar(x_pos + width/2, eval_df['Precision (Weighted)'], width, label='Weighted', alpha=0.8)\n",
    "    axes[0,1].set_title('Precision Comparison', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Models')\n",
    "    axes[0,1].set_ylabel('Precision (%)')\n",
    "    axes[0,1].set_xticks(x_pos)\n",
    "    axes[0,1].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Recall Comparison\n",
    "    axes[1,0].bar(x_pos - width/2, eval_df['Recall (Macro)'], width, label='Macro', alpha=0.8)\n",
    "    axes[1,0].bar(x_pos + width/2, eval_df['Recall (Weighted)'], width, label='Weighted', alpha=0.8)\n",
    "    axes[1,0].set_title('Recall Comparison', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Models')\n",
    "    axes[1,0].set_ylabel('Recall (%)')\n",
    "    axes[1,0].set_xticks(x_pos)\n",
    "    axes[1,0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Performance Heatmap\n",
    "    metrics_for_heatmap = eval_df.set_index('Model')[['Accuracy', 'Precision (Macro)', 'Recall (Macro)',\n",
    "                                                      'F1-Score (Macro)', 'Precision (Weighted)',\n",
    "                                                      'Recall (Weighted)', 'F1-Score (Weighted)']]\n",
    "    sns.heatmap(metrics_for_heatmap.T, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[1,1],\n",
    "                cbar_kws={'label': 'Performance (%)'})\n",
    "    axes[1,1].set_title('Performance Metrics Heatmap', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Models')\n",
    "    axes[1,1].set_ylabel('Metrics')\n",
    "\n",
    "    plt.suptitle('Comprehensive Model Evaluation Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save evaluation metrics to CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    eval_metrics_file = os.path.join(results_dir, f'model_evaluation_metrics_{timestamp}.csv')\n",
    "    eval_df.to_csv(eval_metrics_file, index=False)\n",
    "    print(f\"Model evaluation metrics saved to: {eval_metrics_file}\")\n",
    "\n",
    "    # Find and display best performing models\n",
    "    best_accuracy = eval_df.loc[eval_df['Accuracy'].idxmax()]\n",
    "    best_f1_macro = eval_df.loc[eval_df['F1-Score (Macro)'].idxmax()]\n",
    "    best_f1_weighted = eval_df.loc[eval_df['F1-Score (Weighted)'].idxmax()]\n",
    "\n",
    "    print(f\"\\n BEST PERFORMING MODELS:\")\n",
    "    print(f\"   Best Accuracy: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.2f}%)\")\n",
    "    print(f\"   Best F1-Macro: {best_f1_macro['Model']} ({best_f1_macro['F1-Score (Macro)']:.2f}%)\")\n",
    "    print(f\"   Best F1-Weighted: {best_f1_weighted['Model']} ({best_f1_weighted['F1-Score (Weighted)']:.2f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"No model evaluation results found. Make sure you've evaluated your models and stored results in 'model_results'.\")\n",
    "\n",
    "print(f\"\\nAll analysis results saved in '{results_dir}' directory\")\n",
    "print(\"Comprehensive model analysis completed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\" LOADING MODELS FROM DISK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# IMPORTANT: Define model classes EXACTLY as they were saved\n",
    "# These match the training cell definitions (cells 42, 48, 54, 60)\n",
    "\n",
    "# Check device availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Use CPU for all models to ensure consistent benchmarking\n",
    "primary_device = torch.device(\"cuda\")\n",
    "fallback_device = torch.device(\"cpu\")\n",
    "print(f\"Primary Device: {primary_device}\")\n",
    "\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = Path(MODEL_DIR)\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "\n",
    "# Map model files to their names\n",
    "model_mapping = {\n",
    "    'resnet18_epoch_100_full.pth': 'ResNet18',\n",
    "    'mobilenet_epoch_100_full.pth': 'MobileNetV2',\n",
    "    'efficientnet_epoch_100_full.pth': 'EfficientNet',\n",
    "    'vgg16_epoch_100_full.pth': 'VGG16',\n",
    "    'alexnet_epoch_100_full.pth': 'AlexNet',\n",
    "    'hybridmodel_epoch_100_full.pth': 'Enhanced-Hybrid-VGG-AlexNet',\n",
    "    'cnn_model_epoch_100_full.pth': 'ProposedCNN'\n",
    "}\n",
    "\n",
    "# Load all available models\n",
    "loaded_models = {}\n",
    "model_devices = {}  # Track which device each model is on\n",
    "\n",
    "print(f\"\\nLoading models from: {MODEL_DIR}\\n\")\n",
    "\n",
    "for model_file, model_name in model_mapping.items():\n",
    "    model_path = MODEL_DIR / model_file\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            print(f\"  Loading {model_name}...\", end=\" \")\n",
    "            model = torch.load(model_path, map_location=primary_device, weights_only=False)\n",
    "            model = model.to(primary_device)\n",
    "            model.eval()\n",
    "            loaded_models[model_name] = model\n",
    "            model_devices[model_name] = primary_device\n",
    "            print(f\"\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "    else:\n",
    "        print(f\"   {model_file} not found\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(loaded_models)} models\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the benchmark\n",
    "if loaded_models:\n",
    "    def measure_inference_performance(model, test_loader, model_name, device, num_runs=100, batch_sizes=[1, 8, 16, 32]):\n",
    "        \"\"\"\n",
    "        Comprehensive inference performance measurement for real-time deployment assessment.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'device': str(device),\n",
    "            'batch_performance': {},\n",
    "            'memory_usage': {}\n",
    "        }\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Get model memory footprint (parameters only)\n",
    "        param_size_mb = sum(p.nelement() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "        results['model_size_mb'] = param_size_mb\n",
    "\n",
    "        # Get a sample batch from test data\n",
    "        sample_batch, _ = next(iter(test_loader))\n",
    "        sample_batch = sample_batch.to(device)\n",
    "\n",
    "        # Test different batch sizes\n",
    "        for batch_size in batch_sizes:\n",
    "            if batch_size > len(sample_batch):\n",
    "                continue\n",
    "\n",
    "            X_batch = sample_batch[:batch_size]\n",
    "\n",
    "            # Memory before inference\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            elif torch.backends.mps.is_available() and str(device) == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "                torch.mps.synchronize()\n",
    "\n",
    "            process = psutil.Process()\n",
    "            mem_before = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                _ = model(X_batch)\n",
    "\n",
    "            if torch.cuda.is_available() and str(device) == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            elif torch.backends.mps.is_available() and str(device) == 'mps':\n",
    "                torch.mps.synchronize()\n",
    "\n",
    "            # Timed runs\n",
    "            inference_times = []\n",
    "            for _ in range(num_runs):\n",
    "                if torch.cuda.is_available() and str(device) == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                elif torch.backends.mps.is_available() and str(device) == 'mps':\n",
    "                    torch.mps.synchronize()\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "                with torch.no_grad():\n",
    "                    _ = model(X_batch)\n",
    "\n",
    "                if torch.cuda.is_available() and str(device) == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                elif torch.backends.mps.is_available() and str(device) == 'mps':\n",
    "                    torch.mps.synchronize()\n",
    "\n",
    "                end_time = time.perf_counter()\n",
    "                inference_times.append((end_time - start_time) * 1000)\n",
    "\n",
    "            mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "            mem_delta = mem_after - mem_before\n",
    "\n",
    "            # Statistics\n",
    "            avg_time = np.mean(inference_times)\n",
    "            std_time = np.std(inference_times)\n",
    "            min_time = np.min(inference_times)\n",
    "            max_time = np.max(inference_times)\n",
    "            p95_time = np.percentile(inference_times, 95)\n",
    "            p99_time = np.percentile(inference_times, 99)\n",
    "\n",
    "            avg_time_per_sample = avg_time / batch_size\n",
    "            throughput = (batch_size * 1000) / avg_time\n",
    "\n",
    "            results['batch_performance'][batch_size] = {\n",
    "                'avg_ms': avg_time,\n",
    "                'std_ms': std_time,\n",
    "                'min_ms': min_time,\n",
    "                'max_ms': max_time,\n",
    "                'p95_ms': p95_time,\n",
    "                'p99_ms': p99_time,\n",
    "                'per_sample_ms': avg_time_per_sample,\n",
    "                'throughput_samples_per_sec': throughput,\n",
    "                'memory_delta_mb': mem_delta\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INFERENCE PERFORMANCE BENCHMARK FOR REAL-TIME DEPLOYMENT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nMeasuring inference time and memory usage across different batch sizes...\")\n",
    "    print(\"This assessment is critical for industrial deployment scenarios.\\n\")\n",
    "\n",
    "    benchmark_results = {}\n",
    "    batch_sizes = [1, 8, 16, 32]\n",
    "\n",
    "    for model_name, model in loaded_models.items():\n",
    "        device = model_devices[model_name]\n",
    "        print(f\"\\n{''*80}\")\n",
    "        print(f\"Benchmarking: {model_name}\")\n",
    "        print(f\"{''*80}\")\n",
    "\n",
    "        try:\n",
    "            results = measure_inference_performance(\n",
    "                model=model,\n",
    "                test_loader=test_loader_enhanced,\n",
    "                model_name=model_name,\n",
    "                device=device,\n",
    "                num_runs=100,\n",
    "                batch_sizes=batch_sizes\n",
    "            )\n",
    "\n",
    "            benchmark_results[model_name] = results\n",
    "\n",
    "            print(f\"\\n  Model Size: {results['model_size_mb']:.2f} MB (parameters only)\")\n",
    "            print(f\"\\n  Batch Performance:\")\n",
    "            print(f\"  {'Batch':>6} | {'Avg (ms)':>10} | {'Std (ms)':>10} | {'P95 (ms)':>10} | {'Per Sample':>12} | {'Throughput':>15} | {'Mem Δ':>10}\")\n",
    "            print(f\"  {'-'*6}-+-{'-'*10}-+-{'-'*10}-+-{'-'*10}-+-{'-'*12}-+-{'-'*15}-+-{'-'*10}\")\n",
    "\n",
    "            for batch_size, perf in results['batch_performance'].items():\n",
    "                print(f\"  {batch_size:>6} | {perf['avg_ms']:>10.3f} | {perf['std_ms']:>10.3f} | \"\n",
    "                      f\"{perf['p95_ms']:>10.3f} | {perf['per_sample_ms']:>10.3f} ms | \"\n",
    "                      f\"{perf['throughput_samples_per_sec']:>12.1f} sps | {perf['memory_delta_mb']:>8.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "\n",
    "    # Comparative Analysis\n",
    "    if benchmark_results:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"COMPARATIVE ANALYSIS FOR DEPLOYMENT\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(\"\\nReal-Time Performance (Batch Size = 1):\")\n",
    "        print(f\"  {'Model':.<35} {'Latency (ms)':>15} {'Throughput (sps)':>20} {'Memory':>12}\")\n",
    "        print(f\"  {'-'*35} {'-'*15} {'-'*20} {'-'*12}\")\n",
    "\n",
    "        realtime_perf = []\n",
    "        for model_name, results in benchmark_results.items():\n",
    "            if 1 in results['batch_performance']:\n",
    "                perf = results['batch_performance'][1]\n",
    "                realtime_perf.append({\n",
    "                    'name': model_name,\n",
    "                    'latency': perf['avg_ms'],\n",
    "                    'throughput': perf['throughput_samples_per_sec'],\n",
    "                    'memory': results['model_size_mb']\n",
    "                })\n",
    "                print(f\"  {model_name:.<35} {perf['avg_ms']:>12.3f} ms {perf['throughput_samples_per_sec']:>17.1f} sps \"\n",
    "                      f\"{results['model_size_mb']:>9.2f} MB\")\n",
    "\n",
    "        if realtime_perf:\n",
    "            fastest = min(realtime_perf, key=lambda x: x['latency'])\n",
    "            highest_throughput = max(realtime_perf, key=lambda x: x['throughput'])\n",
    "            smallest = min(realtime_perf, key=lambda x: x['memory'])\n",
    "\n",
    "            print(\"\\n Recommendations for Real-Time Deployment:\")\n",
    "            print(f\"  • Lowest Latency:     {fastest['name']} ({fastest['latency']:.3f} ms)\")\n",
    "            print(f\"  • Highest Throughput: {highest_throughput['name']} ({highest_throughput['throughput']:.1f} samples/sec)\")\n",
    "            print(f\"  • Smallest Memory:    {smallest['name']} ({smallest['memory']:.2f} MB)\")\n",
    "\n",
    "            print(\"\\n Industrial Deployment Guidelines:\")\n",
    "            print(\"  \")\n",
    "            print(\"   Real-time (< 10ms):      Edge devices, immediate response       \")\n",
    "            print(\"   Near real-time (< 50ms): Quality control, inline inspection     \")\n",
    "            print(\"   Batch (< 100ms):         Offline analysis, periodic monitoring  \")\n",
    "            print(\"  \")\n",
    "\n",
    "            print(\"\\n  Model Classification for Deployment:\")\n",
    "            for item in realtime_perf:\n",
    "                latency = item['latency']\n",
    "                if latency < 10:\n",
    "                    category = \"Real-time capable\"\n",
    "                elif latency < 50:\n",
    "                    category = \" Near real-time\"\n",
    "                elif latency < 100:\n",
    "                    category = \" Batch processing\"\n",
    "                else:\n",
    "                    category = \" May need optimization\"\n",
    "                print(f\"    {item['name']:.<35} {category}\")\n",
    "\n",
    "        print(\"\\n Batch Processing Performance (Batch Size = 32):\")\n",
    "        if any(32 in results['batch_performance'] for results in benchmark_results.values()):\n",
    "            print(f\"  {'Model':.<35} {'Total (ms)':>12} {'Per Sample (ms)':>17} {'Throughput (sps)':>20}\")\n",
    "            print(f\"  {'-'*35} {'-'*12} {'-'*17} {'-'*20}\")\n",
    "\n",
    "            for model_name, results in benchmark_results.items():\n",
    "                if 32 in results['batch_performance']:\n",
    "                    perf = results['batch_performance'][32]\n",
    "                    print(f\"  {model_name:.<35} {perf['avg_ms']:>12.3f} {perf['per_sample_ms']:>17.3f} \"\n",
    "                          f\"{perf['throughput_samples_per_sec']:>17.1f} sps\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Inference Benchmarking Complete!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nBenchmarked {len(benchmark_results)} models successfully\")\n",
    "        print(\"   Results saved in 'benchmark_results' variable for further analysis\")\n",
    "    else:\n",
    "        print(\"\\n No models successfully benchmarked.\")\n",
    "else:\n",
    "    print(\"\\n No models could be loaded from disk\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "enhanced_hybrid_integration": {
   "description": "Enhanced hybrid model with CBAM attention and progressive training",
   "timestamp": "2025-08-11T15:02:17.795049",
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
